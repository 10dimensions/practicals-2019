{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practical 4a: Recurrent Neural Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2019/blob/master/RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9jDBz0IbW3Xy"
      },
      "source": [
        "# Practical 4a: Recurrent Neural Networks (RNNs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-0F3Ao8BKa0g"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Feedforward models (eg deep MLPs and ConvNets) map fixed-size input-data (vectors of a fixed dimensionality) to their output labels. They're very powerful and have been successfully used for many tasks. However, a lot of data is not in the form of fixed-size vectors, but exists in the form of **sequences**. Language is one good example, where sentences are sequences of words. In some way, almost any data types can be considered as a sequence (for instance an image consists of a sequence of pixels, speech a sequence of phonemes, and so forth). \n",
        "\n",
        "Recurrent neural networks (**RNNs**) were designed to be able to handle sequential data, and in this practical we will take a closer look at RNNs and then build a model that can generate English sentences in the style of Shakespeare!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "otAAvBVFSZy8"
      },
      "source": [
        "## Learning Objectives\n",
        "* Understand how RNNs model sequential data.\n",
        "* Understand how the vanilla RNN is a generalization of feedforward models to incorporate sequential dependencies.\n",
        "* Understand the issues involved when training RNNs.\n",
        "* Know how to implement an RNN for time-series estimation (**regression**) and an RNN language model (character-level **classification**) in Tensorflow using Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gfKcEFUxa--9"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h8glXxcyew17",
        "colab": {}
      },
      "source": [
        "#@title Imports (RUN ME!) { display-mode: \"form\" }\n",
        "\n",
        "#!pip install tensorflow-gpu==2.0.0-beta0 > /dev/null 2>&1\n",
        "#!pip -q install pydot_ng > /dev/null 2>&1\n",
        "#!pip -q install grapfrffhviz > /dev/null 2>&1\n",
        "#!apt install graphviz > /dev/null 2>&1\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import random\n",
        "import ssl\n",
        "import sys\n",
        "import urllib\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yVL1OwL7aH8c"
      },
      "source": [
        "##From Feedforward to Recurrent Models\n",
        "\n",
        "### Intuition\n",
        "RNNs generalize feedforward networks (FFNs) to be able to work with sequential data. FFNs take an input (e.g. an image) and immediately produce an output (e.g. a digit class), something like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NqZsIaRU6-WK",
        "colab": {}
      },
      "source": [
        "def ffn_forward(x, W_xh, W_ho, b_hid, b_out):\n",
        "\n",
        "    # Compute activations on the hidden layer.\n",
        "    hidden_layer = act_fn(np.dot(W_xh, x) + b_hid)\n",
        "\n",
        "    # Compute the (linear) output layer activations.\n",
        "    output = np.dot(W_ho, hidden_layer) + b_out\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kb3Tjms06_XL"
      },
      "source": [
        "**NOTE**: You don't have to run this cell, it's just shown to illustrate the point.\n",
        "\n",
        "RNNs, on the other hand, consider the data sequentially, and can remember what they have seen earlier in the sequence to help interpret or contextualize elements from later in the sequence when making predictions, something like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ACx_wHGB7AWc",
        "colab": {}
      },
      "source": [
        "def rnn_forward(data_sequence, initial_state):\n",
        "\n",
        "    state = initial_state  # Reused at every time-step\n",
        "    all_states = [state]  # Used to save all states\n",
        "    all_ys = [] # Used to save all predictions\n",
        "    \n",
        "    for x in data_sequence:\n",
        "\n",
        "      # recurrent_fn() takes the current input and the previous state and produces a new state\n",
        "      # it is a place holder which could represent various kinds of recurrent connections \n",
        "      # including vanilla RNN, GRU or LSTM (if you don't know what these mean yet don't worry)\n",
        "      new_state, y_pred = recurrent_fn(x, state)\n",
        "\n",
        "      all_states.append(new_state)\n",
        "      all_ys.append(y_pred)\n",
        "\n",
        "      # Update state for the next time-step\n",
        "      state = new_state\n",
        "\n",
        "    return all_states, all_ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Crh_ViE7Ave"
      },
      "source": [
        "To understand the distinction between FFNs and RNNs, imagine we want to label words as the part-of-speech categories that they belong to: E.g. for the input sentence \"I want a duck\" and \"He had to duck\", we want our model to predict that duck is a `Noun` in the first sentence and a `Verb` in the second. To do this successfully, the model needs to be aware of the surrounding context. However, if we feed a FFN model only one word at a time, how could it know the difference? If we want to feed it all the words at once, how do we deal with the fact that sentences are of different lengths?\n",
        "\n",
        "RNNs solve this issue by processing the sentence word-by-word, and maintaining an internal **state** summarizing what it has seen so far. This applies not only to words, but also to phonemes in speech, or even, as we will see, elements of a time-series.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zUqww79L6Ot-"
      },
      "source": [
        "### Unrolling the network\n",
        "\n",
        "Imagine we are trying to classify a sequence $X = (x_1, x_2, \\ldots, x_N)$ into labels $y$ (for now, let's keep it abstract). After running the `rnn_forward()` function of our RNN defined above on $X$, we would have a list of internal states and outputs of the model at each sequence position. This process is called **unrolling or unfolding in time**, because you can think of it as unrolling the *computations* defined by the RNN loop over the inputs at each position of the sequence.  RNNs are often used to model **time series data** (which we will do in this practical), and therefore these positions are referred to as **time-steps**, and hence we call this process \"unrolling over time\". See the visualization below from this great [blog post](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) (it uses slightly different notation, but the idea should be clear):\n",
        "\n",
        "![Unrolled Network](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg)\n",
        "\n",
        "> **We can therefore think of an RNN as a composition of identical feedforward neural networks (with replicated/tied weights), one for each moment or step in time. **\n",
        "\n",
        "These feedforward functions that make up the RNN(i.e. our `recurrent_fn` above) are typically referred to as **cells**, and the only restriction on its API is that the cell function needs to be a differentiable function that can map an input and a state vector to an output and a new state vector. What we have shown above is called the **vanilla RNN**, but there are many more possibilities. One of the most popular variants is called the **Long Short-Term Memory (LSTM)** cell, which we'll use later to create our Shakespeare language model.\n",
        "\n",
        "### Putting this together \n",
        "\n",
        "In the feedforward models we've seen before, the input $x$ is mapped to an intermediate hidden layer $h$ as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "  h = \\sigma(\\underbrace{W_{xh}x}_\\text{current input (per-example)} + b)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\sigma$ is some non-linear activation function like ReLU or tanh.  We can then make a prediction $\\hat{y} = \\sigma(W_{hy}h + b)$ based on $h$, or we can add another layer, etc.  **NOTE**: We use the weight subscript $W_{xz}$ to indicate a mapping from layer $x$ to layer $z$.\n",
        "\n",
        "RNNs generalize this idea to a sequence of inputs $X = {x_1, x_2, ...}$ by maintaining a sequence of state vectors $h_t$, one for every time-step $t$, as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "  h_t = \\sigma(\\underbrace{W_{hh}h_{t-1}}_\\text{previous state} + \\underbrace{W_{xh}x_t}_\\text{current input (per time-step)} + b)\n",
        "\\end{equation}\n",
        "\n",
        "Feedforward models map one input to one output. On the other hand, RNNs can give **many-to-many** (many inputs, many labels), **many-to-one**, and **one-to-many** mappings. Examples of these tasks include machine translation, sentiment analysis, and image captioning, respectively. For example, we could use each $h_t$ to predict the part of speech for that time-step ($y_t= \\sigma(W_{hy}h_t+b)$) or we could use the last state $h_T$ to predict the topic of the whole document. Which task the RNN performs is based on the training data (of course)! We can visualize these as follows (from this [excellent blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy):\n",
        "\n",
        "![](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "**QUESTIONS**\n",
        "* How are FFNs and RNNs **similar**?\n",
        "* How are they **different**?\n",
        "* Why do we call RNNs \"recurrent\"?\n",
        "* Can you think of a one-to-many task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9wuobtH3aB59"
      },
      "source": [
        "##Modeling General Time-Series\n",
        "\n",
        "We will train an RNN to model a time-series as a first step. A **time-series** is a series of data-points ordered over discrete time-steps. Examples include the hourly temperature of Stellenbosch over a month or a year, the market price of some asset (like a company's stock) over time, and so forth. We will generate a **sinusoidal time-series** (with or without noise) as a toy example, and then train a tiny RNN model with only 5 parameters on this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oAQIXUL-oje2"
      },
      "source": [
        "###Create some artificial data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "412j4v-RIfYR",
        "outputId": "e5e57111-6884-47ce-c85d-adcb13c433d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "#@title Create sinusoidal data {run: \"auto\"}\n",
        "steps_per_cycle = 20 #@param { type: \"slider\", min:1, max:100, step:1 }\n",
        "number_of_cycles = 176 #@param { type: \"slider\", min:1, max:1000, step:1 }\n",
        "noise_factor = 0.1 #@param { type: \"slider\", min:0, max:1, step:0.1 }\n",
        "plot_num_cycles = 23 #@param { type: \"slider\", min:1, max:50, step:1 }\n",
        "\n",
        "seq_len = steps_per_cycle * number_of_cycles\n",
        "t = np.arange(seq_len)\n",
        "sin_t_noisy = np.sin(2 * np.pi / steps_per_cycle * t + noise_factor * np.random.uniform(-1.0, +1.0, seq_len))\n",
        "sin_t_clean = np.sin(2 * np.pi / steps_per_cycle * t)\n",
        "\n",
        "upto = plot_num_cycles * steps_per_cycle\n",
        "fig = plt.figure(figsize=(15,3))\n",
        "plt.plot(t[:upto], sin_t_noisy[:upto])\n",
        "plt.title(\"Showing first {} cycles.\".format(plot_num_cycles))\n",
        "plt.show()\n",
        "\n",
        "#both = np.column_stack((t, sin_t_noisy))\n",
        "#print(\"both.shape = {}\".format(both.shape))\n",
        "\n",
        "#print(\"both[:steps_per_cycle, :steps_per_cycle]\")\n",
        "#print(both[:steps_per_cycle,:steps_per_cycle])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAADSCAYAAAAGyFLoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eaypg9gKos21"
      },
      "source": [
        "**TASK**: Adjust the parameters above to generate data with different properties.\n",
        "\n",
        "Now we pack the data into train and test batches. Note that while RNNs can in theory learn the dependencies across all inputs received so far (using an algorithm called **backpropagation through time**, or BPTT; see the Aside box below), in practice they are trained using an algorithm called **truncated BPTT** where we truncate the inputs to only the last $T$ symbols (this is the `truncated_seq_len` variable below).\n",
        "\n",
        "**QUESTIONs**: \n",
        "* What issues can you think may arise by truncating the training data in this way? \n",
        "* Despite these issues, why do you think it might be necessary to do this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "RtwMLLCNorw4",
        "outputId": "346aaf65-a3f1-427c-ec0d-befb061a0aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#@title Pack truncated sequence data {run: \"auto\"}\n",
        "\n",
        "def pack_truncated_data(data, num_prev = 100):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - num_prev):\n",
        "      X.append(data[i : i + num_prev])\n",
        "      Y.append(data[i + num_prev])\n",
        "    # NOTE: Keras expects input data in the shape (batch_size, truncated_seq_len, input_dim)\n",
        "    # We have only one real-valued number per time-step, so we therefore expand\n",
        "    # the last dimension from (batch_size, truncated_seq_len) to\n",
        "    # (batch_size, truncated_seq_len, 1).\n",
        "    X, Y = np.array(X)[:,:,np.newaxis], np.array(Y)[:,np.newaxis]\n",
        "    return X, Y\n",
        "\n",
        "# We only consider this many previous data points\n",
        "truncated_seq_len = 2 #@param { type: \"slider\", min:1, max:10, step:1 }\n",
        "test_split = 0.25  # Fraction of total data to keep out as test data\n",
        "\n",
        "# We use only the sin(t) values, and discard the time values\n",
        "data = sin_t_noisy\n",
        "data_len = data.shape[0]\n",
        "num_train = int(data_len * (1 - test_split))\n",
        "\n",
        "train_data = data[:num_train]\n",
        "test_data = data[num_train:]\n",
        "\n",
        "X_train, y_train = pack_truncated_data(train_data, num_prev=truncated_seq_len)\n",
        "X_test, y_test = pack_truncated_data(test_data, num_prev=truncated_seq_len)\n",
        "\n",
        "print(\"Generated training/test data with shapes\\nX_train: {}, y_train: {}\\nX_test: {}, y_test: {}. \".format(\n",
        "    X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated training/test data with shapes\n",
            "X_train: (2638, 2, 1), y_train: (2638, 1)\n",
            "X_test: (878, 2, 1), y_test: (878, 1). \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vgVxLBp8CaN6"
      },
      "source": [
        "**NOTE**: We reshape the training data into (batch_size, truncated_seq_len, 1) and (batch_size, 1) arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xiUrsPAI36M-"
      },
      "source": [
        "### Intermediate optional extra reading: (Truncated) Backpropagation-through-Time and Vanishing and Exploding Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SCJlN3O11pr1"
      },
      "source": [
        "RNNs model sequential data, and are designed to capture how ***outputs*** at the current time step are influenced by the ***inputs*** that came before them. This is referred to as **long-range dependencies**. At a high level, this allows the model to remember what it has seen so far in order to better contextualize what it is seeing at the moment (think about how knowing the context of the sentence or conversation can sometimes help one to better figure out the intended meaning of a misheard word or ambiguous statement). It is what makes these models so powerful, but it is also what makes them so hard to train!\n",
        "\n",
        "The most well-known algorithm for training RNNs is called **back-propagation through time (BPTT**; there are other algorithms). BPTT conceptually amounts to unrolling the computations of the RNN over time, computing the errors, and backpropagating the gradients through the unrolled graph structure.  Ideally we want to unroll the graph up to the maximum sequence length, however in practice, since sequence lengths vary and memory is limited, we only end up unrolling sequences up to some length $T$. This is called **truncated BPTT**, and is the most used variant of BPTT.\n",
        "\n",
        "At a high level, there are two main issues when using (truncated) BPTT to train RNNs:\n",
        "\n",
        "* Having shared (\"tied\") recurrent weights ($W_{hh}$) mean that **the gradient on these weights at some time step $t$ depends on all time steps up to time-step $T$**, the length of the full (truncated) sequence. This also leads to the **vanishing/exploding gradients** problem, which we'll explain below.\n",
        "\n",
        "* **Memory usage grows linearly with the total number of steps $T$ that we unroll for**, because we need to save/cache the activations at each time-step (look at the Python code above to convince yourself of this).  This matters computationally, since memory is a limited resource. It also matters statistically, because it puts a limit on the types of dependency that the model can successfully learn, by preventing it from correcting errors that stem from an input more than $T$ steps ago.\n",
        "\n",
        "**NOTE**: Think about that last statement and make sure you understand those 2 points.\n",
        "\n",
        "BPTT is very similar to the standard back-propagation algorithm. Key to understanding the BPTT algorithm is to realize that gradients on the non-recurrent weights (weights of a per time-step classifier that tries to predict the part-of-speech tag for each word for example) and recurrent weights (that transform $h_{t-1}$ into $h_t$) are computed differently:\n",
        "\n",
        "* The gradients of **non-recurrent weights** ($W_{hy}$) depend only on the error at that time-step, $E_t$.\n",
        "* The gradients of **recurrent weights** ($W_{hh}$) depend on all previous time-steps up to maximum length $T$.\n",
        "\n",
        "The first point is fairly intuitive: predictions at time-step $t$ is related to the loss of that particular prediction. \n",
        "\n",
        "The second point will be explained in more detail in the lectures (see also [this great blog post](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)), but briefly, this can be summarized in these equations:\n",
        "\n",
        "1. The **current** state is a function of the **previous** state and the current input: $h_t = \\sigma(W_{hh}h_{t-1} + W_{xh}x_t)$\n",
        "2. The gradient of the loss $E_t$ at time $t$ on $W_{hh}$ is a function of the current hidden state and model predictions $\\hat{y}_t$ at time t: \n",
        "$\\frac{\\partial E_t}{\\partial W_{hh}} = \\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial W_{hh}}$\n",
        "3. Substituting (1) into (2) results in a **sum over all previous time-steps**:\n",
        "$\\frac{\\partial E_t}{\\partial W_{hh}} = \\sum\\limits_{k=0}^{t} \\underbrace{\\frac{\\partial E_t}{\\partial \\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial h_k}\\frac{\\partial h_k}{\\partial W_{hh}}}_\\text{product of gradient terms}$\n",
        "\n",
        "The problem is that $\\frac{\\partial h_t}{\\partial h_k} = \\Pi_j \\frac{\\partial h_j}{\\partial h_{j-1}}$ for j from $k + 1$ to $t$.  Because of this **repeated multiplicative interaction**, as the sequence length $t$ gets longer, the gradients themselves can get diminishingly small (**vanish**) or grow too large and result in numeric overflow (**explode**). This has been shown to be related to the norms of the recurrent weight matrices being less than or equal to 1. Intuitively, it works very similar to how multiplying a small number $v<1.0$ with itself repeatedly can quickly go to zero, or conversely, a large number $v>1.0$ could quickly go to infinity; only this is for matrices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-o3qjqeZpLjN"
      },
      "source": [
        "###Build a tiny RNN in Keras\n",
        "\n",
        "Building an RNN in Keras is quite simple. We simply chain the layers together as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mEobTD6spOWx",
        "colab": {}
      },
      "source": [
        "def define_model(truncated_seq_len):\n",
        "\n",
        "    input_dimension = 1\n",
        "    hidden_dimension = 1\n",
        "    output_dimension = 1\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.SimpleRNN(\n",
        "        # We need to specify the input_shape *without* leading batch_size (it is inferred)\n",
        "        input_shape=(truncated_seq_len, input_dimension),\n",
        "        units=hidden_dimension,\n",
        "        return_sequences=False,\n",
        "        name='hidden_layer'))\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        output_dimension,\n",
        "        name='output_layer'))\n",
        "\n",
        "    model.compile(loss=\"mean_squared_error\",\n",
        "                  optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3))\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JHRscZaQze26"
      },
      "source": [
        "**NOTE**: We're building an RNN for **regression**. We therefore use a linear layer (which outputs real-valued numbers) at the output with the \"*mean_squared_error*\" loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ONvBy4AopTrF",
        "outputId": "b45163ce-91d2-459e-b1c3-6847d753a2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "model = define_model(truncated_seq_len = X_train.shape[1])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0708 18:29:35.965700 139985615468416 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer (SimpleRNN)     (None, 1)                 3         \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ddb6_04dfZvn"
      },
      "source": [
        "**NOTE**: You need to re-run the above cell every time after training to reset the model weights!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hD94X5iQc8Jg"
      },
      "source": [
        "###Train the tiny RNN\n",
        "Now let's train the model. This may take a few minutes (it takes much longer if you increase `truncated_seq_len`). Set `verbose=1` **before** you run the cell to see the intermediate output as the model is training. Set it to 0 if you don't want any output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xvahCyhk7Wvr",
        "outputId": "4546a87b-bb39-400a-f838-4bc138a3a40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "''' SOLUTION TO ONE OF TASKS [DELETE]\n",
        "patience = 5\n",
        "train_history = model.fit(X_train, y_train, batch_size=600, epochs=1000,\n",
        "                          verbose=1, validation_split=0.05,\n",
        "                          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1)])\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" SOLUTION TO ONE OF TASKS [DELETE]\\npatience = 5\\ntrain_history = model.fit(X_train, y_train, batch_size=600, epochs=1000,\\n                          verbose=1, validation_split=0.05,\\n                          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1)])\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xumvRz2lrPus",
        "outputId": "3a0e6efe-cfb8-40e6-c3df-c473fb2e1d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_history = model.fit(X_train, y_train, batch_size=600, epochs=1000,\n",
        "                          verbose=1, validation_split=0.05)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2506 samples, validate on 132 samples\n",
            "Epoch 1/1000\n",
            "2506/2506 [==============================] - 4s 1ms/sample - loss: 0.9979 - val_loss: 0.9847\n",
            "Epoch 2/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.9882 - val_loss: 0.9755\n",
            "Epoch 3/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.9788 - val_loss: 0.9662\n",
            "Epoch 4/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.9694 - val_loss: 0.9569\n",
            "Epoch 5/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.9601 - val_loss: 0.9476\n",
            "Epoch 6/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.9509 - val_loss: 0.9385\n",
            "Epoch 7/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.9419 - val_loss: 0.9296\n",
            "Epoch 8/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.9330 - val_loss: 0.9210\n",
            "Epoch 9/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.9243 - val_loss: 0.9126\n",
            "Epoch 10/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.9157 - val_loss: 0.9043\n",
            "Epoch 11/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.9073 - val_loss: 0.8961\n",
            "Epoch 12/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.8990 - val_loss: 0.8881\n",
            "Epoch 13/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.8909 - val_loss: 0.8801\n",
            "Epoch 14/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.8829 - val_loss: 0.8722\n",
            "Epoch 15/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.8751 - val_loss: 0.8644\n",
            "Epoch 16/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.8674 - val_loss: 0.8567\n",
            "Epoch 17/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.8598 - val_loss: 0.8492\n",
            "Epoch 18/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.8523 - val_loss: 0.8418\n",
            "Epoch 19/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.8449 - val_loss: 0.8345\n",
            "Epoch 20/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.8377 - val_loss: 0.8274\n",
            "Epoch 21/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.8307 - val_loss: 0.8205\n",
            "Epoch 22/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.8237 - val_loss: 0.8137\n",
            "Epoch 23/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.8170 - val_loss: 0.8071\n",
            "Epoch 24/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.8104 - val_loss: 0.8006\n",
            "Epoch 25/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.8039 - val_loss: 0.7941\n",
            "Epoch 26/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.7974 - val_loss: 0.7878\n",
            "Epoch 27/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.7911 - val_loss: 0.7816\n",
            "Epoch 28/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.7849 - val_loss: 0.7755\n",
            "Epoch 29/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.7787 - val_loss: 0.7695\n",
            "Epoch 30/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.7728 - val_loss: 0.7637\n",
            "Epoch 31/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.7669 - val_loss: 0.7581\n",
            "Epoch 32/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.7612 - val_loss: 0.7524\n",
            "Epoch 33/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.7555 - val_loss: 0.7470\n",
            "Epoch 34/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.7499 - val_loss: 0.7416\n",
            "Epoch 35/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.7444 - val_loss: 0.7361\n",
            "Epoch 36/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.7390 - val_loss: 0.7307\n",
            "Epoch 37/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.7337 - val_loss: 0.7255\n",
            "Epoch 38/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.7285 - val_loss: 0.7203\n",
            "Epoch 39/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.7234 - val_loss: 0.7151\n",
            "Epoch 40/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.7185 - val_loss: 0.7101\n",
            "Epoch 41/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.7136 - val_loss: 0.7052\n",
            "Epoch 42/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.7088 - val_loss: 0.7004\n",
            "Epoch 43/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.7042 - val_loss: 0.6958\n",
            "Epoch 44/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.6996 - val_loss: 0.6915\n",
            "Epoch 45/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.6951 - val_loss: 0.6871\n",
            "Epoch 46/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6907 - val_loss: 0.6828\n",
            "Epoch 47/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.6864 - val_loss: 0.6785\n",
            "Epoch 48/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6821 - val_loss: 0.6743\n",
            "Epoch 49/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6780 - val_loss: 0.6702\n",
            "Epoch 50/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.6739 - val_loss: 0.6661\n",
            "Epoch 51/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6699 - val_loss: 0.6622\n",
            "Epoch 52/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6660 - val_loss: 0.6583\n",
            "Epoch 53/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.6622 - val_loss: 0.6546\n",
            "Epoch 54/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6585 - val_loss: 0.6508\n",
            "Epoch 55/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6548 - val_loss: 0.6472\n",
            "Epoch 56/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.6512 - val_loss: 0.6437\n",
            "Epoch 57/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6477 - val_loss: 0.6402\n",
            "Epoch 58/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6442 - val_loss: 0.6369\n",
            "Epoch 59/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6408 - val_loss: 0.6336\n",
            "Epoch 60/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6374 - val_loss: 0.6303\n",
            "Epoch 61/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6342 - val_loss: 0.6272\n",
            "Epoch 62/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6309 - val_loss: 0.6240\n",
            "Epoch 63/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.6278 - val_loss: 0.6208\n",
            "Epoch 64/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6247 - val_loss: 0.6176\n",
            "Epoch 65/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6217 - val_loss: 0.6145\n",
            "Epoch 66/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6187 - val_loss: 0.6115\n",
            "Epoch 67/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.6159 - val_loss: 0.6087\n",
            "Epoch 68/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.6130 - val_loss: 0.6060\n",
            "Epoch 69/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.6102 - val_loss: 0.6033\n",
            "Epoch 70/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.6075 - val_loss: 0.6007\n",
            "Epoch 71/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.6049 - val_loss: 0.5981\n",
            "Epoch 72/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.6023 - val_loss: 0.5957\n",
            "Epoch 73/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5998 - val_loss: 0.5933\n",
            "Epoch 74/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5973 - val_loss: 0.5909\n",
            "Epoch 75/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5949 - val_loss: 0.5886\n",
            "Epoch 76/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5925 - val_loss: 0.5864\n",
            "Epoch 77/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5901 - val_loss: 0.5841\n",
            "Epoch 78/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5878 - val_loss: 0.5819\n",
            "Epoch 79/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.5856 - val_loss: 0.5796\n",
            "Epoch 80/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5834 - val_loss: 0.5775\n",
            "Epoch 81/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5812 - val_loss: 0.5753\n",
            "Epoch 82/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5790 - val_loss: 0.5733\n",
            "Epoch 83/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5770 - val_loss: 0.5715\n",
            "Epoch 84/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5749 - val_loss: 0.5696\n",
            "Epoch 85/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5730 - val_loss: 0.5675\n",
            "Epoch 86/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.5710 - val_loss: 0.5656\n",
            "Epoch 87/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5691 - val_loss: 0.5636\n",
            "Epoch 88/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5672 - val_loss: 0.5616\n",
            "Epoch 89/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5653 - val_loss: 0.5597\n",
            "Epoch 90/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5635 - val_loss: 0.5579\n",
            "Epoch 91/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5618 - val_loss: 0.5562\n",
            "Epoch 92/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5600 - val_loss: 0.5545\n",
            "Epoch 93/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5583 - val_loss: 0.5528\n",
            "Epoch 94/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5567 - val_loss: 0.5510\n",
            "Epoch 95/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5550 - val_loss: 0.5494\n",
            "Epoch 96/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5534 - val_loss: 0.5479\n",
            "Epoch 97/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5518 - val_loss: 0.5463\n",
            "Epoch 98/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.5503 - val_loss: 0.5449\n",
            "Epoch 99/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5488 - val_loss: 0.5435\n",
            "Epoch 100/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5473 - val_loss: 0.5421\n",
            "Epoch 101/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5459 - val_loss: 0.5408\n",
            "Epoch 102/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5445 - val_loss: 0.5393\n",
            "Epoch 103/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5431 - val_loss: 0.5379\n",
            "Epoch 104/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5417 - val_loss: 0.5365\n",
            "Epoch 105/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5404 - val_loss: 0.5353\n",
            "Epoch 106/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5391 - val_loss: 0.5340\n",
            "Epoch 107/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5378 - val_loss: 0.5327\n",
            "Epoch 108/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5365 - val_loss: 0.5316\n",
            "Epoch 109/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5353 - val_loss: 0.5304\n",
            "Epoch 110/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5340 - val_loss: 0.5292\n",
            "Epoch 111/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5328 - val_loss: 0.5280\n",
            "Epoch 112/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5317 - val_loss: 0.5268\n",
            "Epoch 113/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5305 - val_loss: 0.5256\n",
            "Epoch 114/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5294 - val_loss: 0.5245\n",
            "Epoch 115/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5283 - val_loss: 0.5233\n",
            "Epoch 116/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.5272 - val_loss: 0.5223\n",
            "Epoch 117/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5261 - val_loss: 0.5212\n",
            "Epoch 118/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5250 - val_loss: 0.5201\n",
            "Epoch 119/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5240 - val_loss: 0.5191\n",
            "Epoch 120/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5230 - val_loss: 0.5182\n",
            "Epoch 121/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5219 - val_loss: 0.5172\n",
            "Epoch 122/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5209 - val_loss: 0.5163\n",
            "Epoch 123/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5200 - val_loss: 0.5155\n",
            "Epoch 124/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5190 - val_loss: 0.5146\n",
            "Epoch 125/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.5181 - val_loss: 0.5137\n",
            "Epoch 126/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5172 - val_loss: 0.5127\n",
            "Epoch 127/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5163 - val_loss: 0.5118\n",
            "Epoch 128/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5154 - val_loss: 0.5109\n",
            "Epoch 129/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5145 - val_loss: 0.5101\n",
            "Epoch 130/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5136 - val_loss: 0.5092\n",
            "Epoch 131/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5128 - val_loss: 0.5083\n",
            "Epoch 132/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5119 - val_loss: 0.5074\n",
            "Epoch 133/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5111 - val_loss: 0.5065\n",
            "Epoch 134/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5102 - val_loss: 0.5056\n",
            "Epoch 135/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.5094 - val_loss: 0.5048\n",
            "Epoch 136/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5086 - val_loss: 0.5041\n",
            "Epoch 137/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5079 - val_loss: 0.5035\n",
            "Epoch 138/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5071 - val_loss: 0.5028\n",
            "Epoch 139/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5063 - val_loss: 0.5020\n",
            "Epoch 140/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5056 - val_loss: 0.5012\n",
            "Epoch 141/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5048 - val_loss: 0.5005\n",
            "Epoch 142/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5041 - val_loss: 0.4998\n",
            "Epoch 143/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.5034 - val_loss: 0.4990\n",
            "Epoch 144/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5027 - val_loss: 0.4983\n",
            "Epoch 145/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5020 - val_loss: 0.4977\n",
            "Epoch 146/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.5013 - val_loss: 0.4970\n",
            "Epoch 147/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.5006 - val_loss: 0.4964\n",
            "Epoch 148/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4999 - val_loss: 0.4957\n",
            "Epoch 149/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4992 - val_loss: 0.4951\n",
            "Epoch 150/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4985 - val_loss: 0.4943\n",
            "Epoch 151/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4978 - val_loss: 0.4936\n",
            "Epoch 152/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4971 - val_loss: 0.4928\n",
            "Epoch 153/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4965 - val_loss: 0.4920\n",
            "Epoch 154/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4958 - val_loss: 0.4913\n",
            "Epoch 155/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4951 - val_loss: 0.4906\n",
            "Epoch 156/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4945 - val_loss: 0.4899\n",
            "Epoch 157/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4938 - val_loss: 0.4892\n",
            "Epoch 158/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4932 - val_loss: 0.4885\n",
            "Epoch 159/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4925 - val_loss: 0.4879\n",
            "Epoch 160/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4919 - val_loss: 0.4873\n",
            "Epoch 161/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.4913 - val_loss: 0.4866\n",
            "Epoch 162/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4906 - val_loss: 0.4861\n",
            "Epoch 163/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4900 - val_loss: 0.4855\n",
            "Epoch 164/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.4894 - val_loss: 0.4850\n",
            "Epoch 165/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4887 - val_loss: 0.4844\n",
            "Epoch 166/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4881 - val_loss: 0.4838\n",
            "Epoch 167/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4875 - val_loss: 0.4832\n",
            "Epoch 168/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4869 - val_loss: 0.4827\n",
            "Epoch 169/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4863 - val_loss: 0.4821\n",
            "Epoch 170/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4857 - val_loss: 0.4815\n",
            "Epoch 171/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4851 - val_loss: 0.4809\n",
            "Epoch 172/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4844 - val_loss: 0.4803\n",
            "Epoch 173/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4838 - val_loss: 0.4797\n",
            "Epoch 174/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4832 - val_loss: 0.4792\n",
            "Epoch 175/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4826 - val_loss: 0.4787\n",
            "Epoch 176/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4820 - val_loss: 0.4782\n",
            "Epoch 177/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4814 - val_loss: 0.4776\n",
            "Epoch 178/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4808 - val_loss: 0.4771\n",
            "Epoch 179/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4802 - val_loss: 0.4764\n",
            "Epoch 180/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4796 - val_loss: 0.4758\n",
            "Epoch 181/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4790 - val_loss: 0.4751\n",
            "Epoch 182/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4783 - val_loss: 0.4744\n",
            "Epoch 183/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4777 - val_loss: 0.4737\n",
            "Epoch 184/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4771 - val_loss: 0.4730\n",
            "Epoch 185/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4764 - val_loss: 0.4723\n",
            "Epoch 186/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4758 - val_loss: 0.4716\n",
            "Epoch 187/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4752 - val_loss: 0.4709\n",
            "Epoch 188/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4745 - val_loss: 0.4701\n",
            "Epoch 189/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4739 - val_loss: 0.4692\n",
            "Epoch 190/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.4733 - val_loss: 0.4684\n",
            "Epoch 191/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4726 - val_loss: 0.4677\n",
            "Epoch 192/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4719 - val_loss: 0.4670\n",
            "Epoch 193/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4713 - val_loss: 0.4663\n",
            "Epoch 194/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4706 - val_loss: 0.4657\n",
            "Epoch 195/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4699 - val_loss: 0.4652\n",
            "Epoch 196/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4692 - val_loss: 0.4645\n",
            "Epoch 197/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4686 - val_loss: 0.4638\n",
            "Epoch 198/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4679 - val_loss: 0.4632\n",
            "Epoch 199/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4672 - val_loss: 0.4626\n",
            "Epoch 200/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4665 - val_loss: 0.4620\n",
            "Epoch 201/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4658 - val_loss: 0.4613\n",
            "Epoch 202/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4651 - val_loss: 0.4607\n",
            "Epoch 203/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4644 - val_loss: 0.4602\n",
            "Epoch 204/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.4637 - val_loss: 0.4596\n",
            "Epoch 205/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4630 - val_loss: 0.4588\n",
            "Epoch 206/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4623 - val_loss: 0.4580\n",
            "Epoch 207/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4615 - val_loss: 0.4572\n",
            "Epoch 208/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4608 - val_loss: 0.4564\n",
            "Epoch 209/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4601 - val_loss: 0.4557\n",
            "Epoch 210/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.4594 - val_loss: 0.4549\n",
            "Epoch 211/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4586 - val_loss: 0.4542\n",
            "Epoch 212/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.4579 - val_loss: 0.4534\n",
            "Epoch 213/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4571 - val_loss: 0.4526\n",
            "Epoch 214/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4564 - val_loss: 0.4518\n",
            "Epoch 215/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4556 - val_loss: 0.4509\n",
            "Epoch 216/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4549 - val_loss: 0.4501\n",
            "Epoch 217/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4541 - val_loss: 0.4492\n",
            "Epoch 218/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4533 - val_loss: 0.4484\n",
            "Epoch 219/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4525 - val_loss: 0.4475\n",
            "Epoch 220/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4517 - val_loss: 0.4467\n",
            "Epoch 221/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4509 - val_loss: 0.4458\n",
            "Epoch 222/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4501 - val_loss: 0.4449\n",
            "Epoch 223/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4493 - val_loss: 0.4442\n",
            "Epoch 224/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4485 - val_loss: 0.4434\n",
            "Epoch 225/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4476 - val_loss: 0.4426\n",
            "Epoch 226/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4468 - val_loss: 0.4418\n",
            "Epoch 227/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4459 - val_loss: 0.4410\n",
            "Epoch 228/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4451 - val_loss: 0.4403\n",
            "Epoch 229/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4442 - val_loss: 0.4396\n",
            "Epoch 230/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4433 - val_loss: 0.4388\n",
            "Epoch 231/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4425 - val_loss: 0.4379\n",
            "Epoch 232/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.4416 - val_loss: 0.4370\n",
            "Epoch 233/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4408 - val_loss: 0.4360\n",
            "Epoch 234/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4399 - val_loss: 0.4350\n",
            "Epoch 235/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4390 - val_loss: 0.4341\n",
            "Epoch 236/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4381 - val_loss: 0.4332\n",
            "Epoch 237/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4372 - val_loss: 0.4323\n",
            "Epoch 238/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4363 - val_loss: 0.4315\n",
            "Epoch 239/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4354 - val_loss: 0.4307\n",
            "Epoch 240/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4345 - val_loss: 0.4299\n",
            "Epoch 241/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4336 - val_loss: 0.4290\n",
            "Epoch 242/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4327 - val_loss: 0.4280\n",
            "Epoch 243/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.4318 - val_loss: 0.4270\n",
            "Epoch 244/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4308 - val_loss: 0.4261\n",
            "Epoch 245/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4299 - val_loss: 0.4252\n",
            "Epoch 246/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4289 - val_loss: 0.4243\n",
            "Epoch 247/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4280 - val_loss: 0.4234\n",
            "Epoch 248/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4270 - val_loss: 0.4224\n",
            "Epoch 249/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4260 - val_loss: 0.4215\n",
            "Epoch 250/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4251 - val_loss: 0.4205\n",
            "Epoch 251/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4241 - val_loss: 0.4195\n",
            "Epoch 252/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4231 - val_loss: 0.4185\n",
            "Epoch 253/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.4222 - val_loss: 0.4175\n",
            "Epoch 254/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4212 - val_loss: 0.4166\n",
            "Epoch 255/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4202 - val_loss: 0.4156\n",
            "Epoch 256/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4192 - val_loss: 0.4147\n",
            "Epoch 257/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4182 - val_loss: 0.4137\n",
            "Epoch 258/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4172 - val_loss: 0.4127\n",
            "Epoch 259/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4162 - val_loss: 0.4118\n",
            "Epoch 260/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4152 - val_loss: 0.4109\n",
            "Epoch 261/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4142 - val_loss: 0.4098\n",
            "Epoch 262/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4132 - val_loss: 0.4087\n",
            "Epoch 263/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4122 - val_loss: 0.4077\n",
            "Epoch 264/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4111 - val_loss: 0.4066\n",
            "Epoch 265/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4101 - val_loss: 0.4056\n",
            "Epoch 266/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4091 - val_loss: 0.4045\n",
            "Epoch 267/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4080 - val_loss: 0.4035\n",
            "Epoch 268/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4069 - val_loss: 0.4024\n",
            "Epoch 269/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4059 - val_loss: 0.4013\n",
            "Epoch 270/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4049 - val_loss: 0.4002\n",
            "Epoch 271/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4038 - val_loss: 0.3991\n",
            "Epoch 272/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4028 - val_loss: 0.3980\n",
            "Epoch 273/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.4018 - val_loss: 0.3969\n",
            "Epoch 274/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.4008 - val_loss: 0.3959\n",
            "Epoch 275/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3997 - val_loss: 0.3949\n",
            "Epoch 276/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3987 - val_loss: 0.3940\n",
            "Epoch 277/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3977 - val_loss: 0.3930\n",
            "Epoch 278/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3967 - val_loss: 0.3921\n",
            "Epoch 279/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3957 - val_loss: 0.3912\n",
            "Epoch 280/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3947 - val_loss: 0.3901\n",
            "Epoch 281/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3937 - val_loss: 0.3891\n",
            "Epoch 282/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3927 - val_loss: 0.3880\n",
            "Epoch 283/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.3916 - val_loss: 0.3868\n",
            "Epoch 284/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3906 - val_loss: 0.3857\n",
            "Epoch 285/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3896 - val_loss: 0.3847\n",
            "Epoch 286/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3886 - val_loss: 0.3836\n",
            "Epoch 287/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3875 - val_loss: 0.3826\n",
            "Epoch 288/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3865 - val_loss: 0.3816\n",
            "Epoch 289/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3855 - val_loss: 0.3806\n",
            "Epoch 290/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3845 - val_loss: 0.3796\n",
            "Epoch 291/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.3835 - val_loss: 0.3785\n",
            "Epoch 292/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.3825 - val_loss: 0.3775\n",
            "Epoch 293/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3815 - val_loss: 0.3766\n",
            "Epoch 294/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3805 - val_loss: 0.3756\n",
            "Epoch 295/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.3795 - val_loss: 0.3747\n",
            "Epoch 296/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3785 - val_loss: 0.3737\n",
            "Epoch 297/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3775 - val_loss: 0.3728\n",
            "Epoch 298/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3765 - val_loss: 0.3718\n",
            "Epoch 299/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3755 - val_loss: 0.3708\n",
            "Epoch 300/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3745 - val_loss: 0.3700\n",
            "Epoch 301/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3735 - val_loss: 0.3690\n",
            "Epoch 302/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.3726 - val_loss: 0.3680\n",
            "Epoch 303/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3716 - val_loss: 0.3671\n",
            "Epoch 304/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3706 - val_loss: 0.3662\n",
            "Epoch 305/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3697 - val_loss: 0.3653\n",
            "Epoch 306/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3687 - val_loss: 0.3643\n",
            "Epoch 307/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3678 - val_loss: 0.3634\n",
            "Epoch 308/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3669 - val_loss: 0.3624\n",
            "Epoch 309/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3659 - val_loss: 0.3614\n",
            "Epoch 310/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3649 - val_loss: 0.3605\n",
            "Epoch 311/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3640 - val_loss: 0.3595\n",
            "Epoch 312/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3630 - val_loss: 0.3586\n",
            "Epoch 313/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3621 - val_loss: 0.3576\n",
            "Epoch 314/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3612 - val_loss: 0.3566\n",
            "Epoch 315/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3603 - val_loss: 0.3557\n",
            "Epoch 316/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3594 - val_loss: 0.3548\n",
            "Epoch 317/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3585 - val_loss: 0.3540\n",
            "Epoch 318/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3576 - val_loss: 0.3533\n",
            "Epoch 319/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3567 - val_loss: 0.3524\n",
            "Epoch 320/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3559 - val_loss: 0.3514\n",
            "Epoch 321/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3549 - val_loss: 0.3504\n",
            "Epoch 322/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3541 - val_loss: 0.3494\n",
            "Epoch 323/1000\n",
            "2506/2506 [==============================] - 0s 15us/sample - loss: 0.3532 - val_loss: 0.3485\n",
            "Epoch 324/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3524 - val_loss: 0.3476\n",
            "Epoch 325/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3515 - val_loss: 0.3466\n",
            "Epoch 326/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.3507 - val_loss: 0.3459\n",
            "Epoch 327/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3498 - val_loss: 0.3451\n",
            "Epoch 328/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3490 - val_loss: 0.3444\n",
            "Epoch 329/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3482 - val_loss: 0.3438\n",
            "Epoch 330/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3473 - val_loss: 0.3430\n",
            "Epoch 331/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3465 - val_loss: 0.3422\n",
            "Epoch 332/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3457 - val_loss: 0.3413\n",
            "Epoch 333/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3449 - val_loss: 0.3406\n",
            "Epoch 334/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3441 - val_loss: 0.3398\n",
            "Epoch 335/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3433 - val_loss: 0.3391\n",
            "Epoch 336/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3426 - val_loss: 0.3383\n",
            "Epoch 337/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3418 - val_loss: 0.3376\n",
            "Epoch 338/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3410 - val_loss: 0.3369\n",
            "Epoch 339/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.3403 - val_loss: 0.3362\n",
            "Epoch 340/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3395 - val_loss: 0.3355\n",
            "Epoch 341/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3388 - val_loss: 0.3349\n",
            "Epoch 342/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3381 - val_loss: 0.3341\n",
            "Epoch 343/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3373 - val_loss: 0.3333\n",
            "Epoch 344/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3367 - val_loss: 0.3324\n",
            "Epoch 345/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3359 - val_loss: 0.3318\n",
            "Epoch 346/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3352 - val_loss: 0.3312\n",
            "Epoch 347/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3345 - val_loss: 0.3306\n",
            "Epoch 348/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3338 - val_loss: 0.3299\n",
            "Epoch 349/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3331 - val_loss: 0.3292\n",
            "Epoch 350/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3324 - val_loss: 0.3284\n",
            "Epoch 351/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3316 - val_loss: 0.3276\n",
            "Epoch 352/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3310 - val_loss: 0.3268\n",
            "Epoch 353/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3303 - val_loss: 0.3261\n",
            "Epoch 354/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3296 - val_loss: 0.3254\n",
            "Epoch 355/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3290 - val_loss: 0.3247\n",
            "Epoch 356/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3283 - val_loss: 0.3241\n",
            "Epoch 357/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3277 - val_loss: 0.3236\n",
            "Epoch 358/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3270 - val_loss: 0.3230\n",
            "Epoch 359/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3264 - val_loss: 0.3224\n",
            "Epoch 360/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.3258 - val_loss: 0.3218\n",
            "Epoch 361/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3252 - val_loss: 0.3211\n",
            "Epoch 362/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.3246 - val_loss: 0.3204\n",
            "Epoch 363/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3240 - val_loss: 0.3198\n",
            "Epoch 364/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.3234 - val_loss: 0.3193\n",
            "Epoch 365/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.3228 - val_loss: 0.3187\n",
            "Epoch 366/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.3222 - val_loss: 0.3183\n",
            "Epoch 367/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3217 - val_loss: 0.3177\n",
            "Epoch 368/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3211 - val_loss: 0.3170\n",
            "Epoch 369/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3205 - val_loss: 0.3164\n",
            "Epoch 370/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3200 - val_loss: 0.3159\n",
            "Epoch 371/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3194 - val_loss: 0.3156\n",
            "Epoch 372/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3189 - val_loss: 0.3151\n",
            "Epoch 373/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3184 - val_loss: 0.3145\n",
            "Epoch 374/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3178 - val_loss: 0.3139\n",
            "Epoch 375/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3173 - val_loss: 0.3134\n",
            "Epoch 376/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3168 - val_loss: 0.3130\n",
            "Epoch 377/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3163 - val_loss: 0.3127\n",
            "Epoch 378/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3158 - val_loss: 0.3123\n",
            "Epoch 379/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3153 - val_loss: 0.3120\n",
            "Epoch 380/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3148 - val_loss: 0.3115\n",
            "Epoch 381/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3143 - val_loss: 0.3111\n",
            "Epoch 382/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3139 - val_loss: 0.3106\n",
            "Epoch 383/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3134 - val_loss: 0.3102\n",
            "Epoch 384/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3129 - val_loss: 0.3096\n",
            "Epoch 385/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3124 - val_loss: 0.3092\n",
            "Epoch 386/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3120 - val_loss: 0.3087\n",
            "Epoch 387/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3115 - val_loss: 0.3081\n",
            "Epoch 388/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3111 - val_loss: 0.3076\n",
            "Epoch 389/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3106 - val_loss: 0.3070\n",
            "Epoch 390/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3102 - val_loss: 0.3066\n",
            "Epoch 391/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3097 - val_loss: 0.3061\n",
            "Epoch 392/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.3093 - val_loss: 0.3056\n",
            "Epoch 393/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3089 - val_loss: 0.3051\n",
            "Epoch 394/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3085 - val_loss: 0.3047\n",
            "Epoch 395/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3080 - val_loss: 0.3043\n",
            "Epoch 396/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3076 - val_loss: 0.3038\n",
            "Epoch 397/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3072 - val_loss: 0.3033\n",
            "Epoch 398/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.3068 - val_loss: 0.3029\n",
            "Epoch 399/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.3064 - val_loss: 0.3027\n",
            "Epoch 400/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3060 - val_loss: 0.3027\n",
            "Epoch 401/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3056 - val_loss: 0.3025\n",
            "Epoch 402/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3052 - val_loss: 0.3021\n",
            "Epoch 403/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3048 - val_loss: 0.3016\n",
            "Epoch 404/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3044 - val_loss: 0.3011\n",
            "Epoch 405/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3040 - val_loss: 0.3006\n",
            "Epoch 406/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3036 - val_loss: 0.3003\n",
            "Epoch 407/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3032 - val_loss: 0.2998\n",
            "Epoch 408/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.3029 - val_loss: 0.2994\n",
            "Epoch 409/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3025 - val_loss: 0.2990\n",
            "Epoch 410/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3021 - val_loss: 0.2986\n",
            "Epoch 411/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.3018 - val_loss: 0.2983\n",
            "Epoch 412/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.3014 - val_loss: 0.2980\n",
            "Epoch 413/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3011 - val_loss: 0.2978\n",
            "Epoch 414/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.3007 - val_loss: 0.2977\n",
            "Epoch 415/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.3004 - val_loss: 0.2973\n",
            "Epoch 416/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.3000 - val_loss: 0.2969\n",
            "Epoch 417/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2997 - val_loss: 0.2964\n",
            "Epoch 418/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2993 - val_loss: 0.2959\n",
            "Epoch 419/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2990 - val_loss: 0.2954\n",
            "Epoch 420/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2987 - val_loss: 0.2950\n",
            "Epoch 421/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2984 - val_loss: 0.2946\n",
            "Epoch 422/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2980 - val_loss: 0.2942\n",
            "Epoch 423/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2977 - val_loss: 0.2939\n",
            "Epoch 424/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2974 - val_loss: 0.2936\n",
            "Epoch 425/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2971 - val_loss: 0.2934\n",
            "Epoch 426/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2968 - val_loss: 0.2931\n",
            "Epoch 427/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2964 - val_loss: 0.2929\n",
            "Epoch 428/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2961 - val_loss: 0.2926\n",
            "Epoch 429/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2958 - val_loss: 0.2924\n",
            "Epoch 430/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2955 - val_loss: 0.2921\n",
            "Epoch 431/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2952 - val_loss: 0.2919\n",
            "Epoch 432/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2949 - val_loss: 0.2919\n",
            "Epoch 433/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2946 - val_loss: 0.2917\n",
            "Epoch 434/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2943 - val_loss: 0.2915\n",
            "Epoch 435/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2941 - val_loss: 0.2913\n",
            "Epoch 436/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2938 - val_loss: 0.2909\n",
            "Epoch 437/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2935 - val_loss: 0.2906\n",
            "Epoch 438/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2932 - val_loss: 0.2903\n",
            "Epoch 439/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2929 - val_loss: 0.2900\n",
            "Epoch 440/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2926 - val_loss: 0.2898\n",
            "Epoch 441/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2923 - val_loss: 0.2896\n",
            "Epoch 442/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2921 - val_loss: 0.2893\n",
            "Epoch 443/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2918 - val_loss: 0.2889\n",
            "Epoch 444/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2915 - val_loss: 0.2886\n",
            "Epoch 445/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2912 - val_loss: 0.2882\n",
            "Epoch 446/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2909 - val_loss: 0.2879\n",
            "Epoch 447/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2907 - val_loss: 0.2876\n",
            "Epoch 448/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2904 - val_loss: 0.2873\n",
            "Epoch 449/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2901 - val_loss: 0.2870\n",
            "Epoch 450/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2899 - val_loss: 0.2869\n",
            "Epoch 451/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2897 - val_loss: 0.2868\n",
            "Epoch 452/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2894 - val_loss: 0.2866\n",
            "Epoch 453/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2891 - val_loss: 0.2865\n",
            "Epoch 454/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2889 - val_loss: 0.2862\n",
            "Epoch 455/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2886 - val_loss: 0.2858\n",
            "Epoch 456/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2883 - val_loss: 0.2855\n",
            "Epoch 457/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2881 - val_loss: 0.2851\n",
            "Epoch 458/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2878 - val_loss: 0.2849\n",
            "Epoch 459/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2876 - val_loss: 0.2845\n",
            "Epoch 460/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2873 - val_loss: 0.2842\n",
            "Epoch 461/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2871 - val_loss: 0.2839\n",
            "Epoch 462/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2868 - val_loss: 0.2837\n",
            "Epoch 463/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2866 - val_loss: 0.2833\n",
            "Epoch 464/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2864 - val_loss: 0.2830\n",
            "Epoch 465/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2861 - val_loss: 0.2828\n",
            "Epoch 466/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2859 - val_loss: 0.2828\n",
            "Epoch 467/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2857 - val_loss: 0.2828\n",
            "Epoch 468/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2854 - val_loss: 0.2826\n",
            "Epoch 469/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2852 - val_loss: 0.2823\n",
            "Epoch 470/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2849 - val_loss: 0.2821\n",
            "Epoch 471/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2847 - val_loss: 0.2818\n",
            "Epoch 472/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2845 - val_loss: 0.2815\n",
            "Epoch 473/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2843 - val_loss: 0.2813\n",
            "Epoch 474/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2840 - val_loss: 0.2811\n",
            "Epoch 475/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2838 - val_loss: 0.2809\n",
            "Epoch 476/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2836 - val_loss: 0.2807\n",
            "Epoch 477/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2834 - val_loss: 0.2804\n",
            "Epoch 478/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.2831 - val_loss: 0.2801\n",
            "Epoch 479/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2830 - val_loss: 0.2797\n",
            "Epoch 480/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2827 - val_loss: 0.2795\n",
            "Epoch 481/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.2825 - val_loss: 0.2794\n",
            "Epoch 482/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2823 - val_loss: 0.2794\n",
            "Epoch 483/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2821 - val_loss: 0.2793\n",
            "Epoch 484/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2818 - val_loss: 0.2793\n",
            "Epoch 485/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2816 - val_loss: 0.2790\n",
            "Epoch 486/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2814 - val_loss: 0.2788\n",
            "Epoch 487/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2812 - val_loss: 0.2784\n",
            "Epoch 488/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2810 - val_loss: 0.2781\n",
            "Epoch 489/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2808 - val_loss: 0.2779\n",
            "Epoch 490/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2806 - val_loss: 0.2777\n",
            "Epoch 491/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2804 - val_loss: 0.2775\n",
            "Epoch 492/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2802 - val_loss: 0.2773\n",
            "Epoch 493/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2800 - val_loss: 0.2769\n",
            "Epoch 494/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2798 - val_loss: 0.2767\n",
            "Epoch 495/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2796 - val_loss: 0.2765\n",
            "Epoch 496/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2794 - val_loss: 0.2765\n",
            "Epoch 497/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2792 - val_loss: 0.2764\n",
            "Epoch 498/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2790 - val_loss: 0.2763\n",
            "Epoch 499/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2788 - val_loss: 0.2762\n",
            "Epoch 500/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2786 - val_loss: 0.2758\n",
            "Epoch 501/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2784 - val_loss: 0.2755\n",
            "Epoch 502/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2782 - val_loss: 0.2754\n",
            "Epoch 503/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2780 - val_loss: 0.2753\n",
            "Epoch 504/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2778 - val_loss: 0.2751\n",
            "Epoch 505/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2776 - val_loss: 0.2747\n",
            "Epoch 506/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2774 - val_loss: 0.2746\n",
            "Epoch 507/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2772 - val_loss: 0.2746\n",
            "Epoch 508/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2771 - val_loss: 0.2747\n",
            "Epoch 509/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2769 - val_loss: 0.2747\n",
            "Epoch 510/1000\n",
            "2506/2506 [==============================] - 0s 15us/sample - loss: 0.2768 - val_loss: 0.2747\n",
            "Epoch 511/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2766 - val_loss: 0.2746\n",
            "Epoch 512/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2764 - val_loss: 0.2742\n",
            "Epoch 513/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2762 - val_loss: 0.2737\n",
            "Epoch 514/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2760 - val_loss: 0.2733\n",
            "Epoch 515/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2757 - val_loss: 0.2729\n",
            "Epoch 516/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2756 - val_loss: 0.2723\n",
            "Epoch 517/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2754 - val_loss: 0.2720\n",
            "Epoch 518/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2753 - val_loss: 0.2719\n",
            "Epoch 519/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2751 - val_loss: 0.2719\n",
            "Epoch 520/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2749 - val_loss: 0.2719\n",
            "Epoch 521/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2747 - val_loss: 0.2720\n",
            "Epoch 522/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2745 - val_loss: 0.2720\n",
            "Epoch 523/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2744 - val_loss: 0.2718\n",
            "Epoch 524/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2742 - val_loss: 0.2715\n",
            "Epoch 525/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2740 - val_loss: 0.2713\n",
            "Epoch 526/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2738 - val_loss: 0.2711\n",
            "Epoch 527/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2737 - val_loss: 0.2710\n",
            "Epoch 528/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2735 - val_loss: 0.2709\n",
            "Epoch 529/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2733 - val_loss: 0.2707\n",
            "Epoch 530/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2731 - val_loss: 0.2705\n",
            "Epoch 531/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2730 - val_loss: 0.2702\n",
            "Epoch 532/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2729 - val_loss: 0.2697\n",
            "Epoch 533/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2727 - val_loss: 0.2695\n",
            "Epoch 534/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2725 - val_loss: 0.2694\n",
            "Epoch 535/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2723 - val_loss: 0.2693\n",
            "Epoch 536/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2722 - val_loss: 0.2691\n",
            "Epoch 537/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2720 - val_loss: 0.2690\n",
            "Epoch 538/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2719 - val_loss: 0.2690\n",
            "Epoch 539/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2717 - val_loss: 0.2688\n",
            "Epoch 540/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2715 - val_loss: 0.2686\n",
            "Epoch 541/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.2714 - val_loss: 0.2684\n",
            "Epoch 542/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2712 - val_loss: 0.2682\n",
            "Epoch 543/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2710 - val_loss: 0.2681\n",
            "Epoch 544/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2709 - val_loss: 0.2680\n",
            "Epoch 545/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2707 - val_loss: 0.2679\n",
            "Epoch 546/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2706 - val_loss: 0.2677\n",
            "Epoch 547/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2704 - val_loss: 0.2674\n",
            "Epoch 548/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2703 - val_loss: 0.2671\n",
            "Epoch 549/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2701 - val_loss: 0.2669\n",
            "Epoch 550/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2700 - val_loss: 0.2668\n",
            "Epoch 551/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2698 - val_loss: 0.2667\n",
            "Epoch 552/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2697 - val_loss: 0.2665\n",
            "Epoch 553/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2695 - val_loss: 0.2663\n",
            "Epoch 554/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2694 - val_loss: 0.2661\n",
            "Epoch 555/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2693 - val_loss: 0.2659\n",
            "Epoch 556/1000\n",
            "2506/2506 [==============================] - 0s 15us/sample - loss: 0.2691 - val_loss: 0.2656\n",
            "Epoch 557/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2690 - val_loss: 0.2655\n",
            "Epoch 558/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2689 - val_loss: 0.2652\n",
            "Epoch 559/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2687 - val_loss: 0.2651\n",
            "Epoch 560/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2686 - val_loss: 0.2651\n",
            "Epoch 561/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2684 - val_loss: 0.2650\n",
            "Epoch 562/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2682 - val_loss: 0.2649\n",
            "Epoch 563/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2681 - val_loss: 0.2649\n",
            "Epoch 564/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2679 - val_loss: 0.2649\n",
            "Epoch 565/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2677 - val_loss: 0.2648\n",
            "Epoch 566/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2676 - val_loss: 0.2647\n",
            "Epoch 567/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2675 - val_loss: 0.2644\n",
            "Epoch 568/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2673 - val_loss: 0.2643\n",
            "Epoch 569/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2672 - val_loss: 0.2642\n",
            "Epoch 570/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2670 - val_loss: 0.2642\n",
            "Epoch 571/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2669 - val_loss: 0.2641\n",
            "Epoch 572/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2667 - val_loss: 0.2639\n",
            "Epoch 573/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2666 - val_loss: 0.2637\n",
            "Epoch 574/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2665 - val_loss: 0.2635\n",
            "Epoch 575/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2663 - val_loss: 0.2635\n",
            "Epoch 576/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2662 - val_loss: 0.2636\n",
            "Epoch 577/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2661 - val_loss: 0.2636\n",
            "Epoch 578/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2659 - val_loss: 0.2634\n",
            "Epoch 579/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2658 - val_loss: 0.2632\n",
            "Epoch 580/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2656 - val_loss: 0.2631\n",
            "Epoch 581/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2655 - val_loss: 0.2629\n",
            "Epoch 582/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2654 - val_loss: 0.2628\n",
            "Epoch 583/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2652 - val_loss: 0.2627\n",
            "Epoch 584/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2651 - val_loss: 0.2626\n",
            "Epoch 585/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2650 - val_loss: 0.2626\n",
            "Epoch 586/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2648 - val_loss: 0.2625\n",
            "Epoch 587/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2647 - val_loss: 0.2626\n",
            "Epoch 588/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2646 - val_loss: 0.2626\n",
            "Epoch 589/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2645 - val_loss: 0.2624\n",
            "Epoch 590/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2643 - val_loss: 0.2620\n",
            "Epoch 591/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2642 - val_loss: 0.2616\n",
            "Epoch 592/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2641 - val_loss: 0.2613\n",
            "Epoch 593/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2639 - val_loss: 0.2611\n",
            "Epoch 594/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2638 - val_loss: 0.2609\n",
            "Epoch 595/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2637 - val_loss: 0.2606\n",
            "Epoch 596/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.2636 - val_loss: 0.2606\n",
            "Epoch 597/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2634 - val_loss: 0.2605\n",
            "Epoch 598/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2634 - val_loss: 0.2606\n",
            "Epoch 599/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2632 - val_loss: 0.2606\n",
            "Epoch 600/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2630 - val_loss: 0.2604\n",
            "Epoch 601/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2629 - val_loss: 0.2601\n",
            "Epoch 602/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2628 - val_loss: 0.2599\n",
            "Epoch 603/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2627 - val_loss: 0.2596\n",
            "Epoch 604/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2626 - val_loss: 0.2595\n",
            "Epoch 605/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2625 - val_loss: 0.2595\n",
            "Epoch 606/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2623 - val_loss: 0.2596\n",
            "Epoch 607/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2622 - val_loss: 0.2597\n",
            "Epoch 608/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2620 - val_loss: 0.2598\n",
            "Epoch 609/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2620 - val_loss: 0.2598\n",
            "Epoch 610/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2618 - val_loss: 0.2596\n",
            "Epoch 611/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2617 - val_loss: 0.2595\n",
            "Epoch 612/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2616 - val_loss: 0.2593\n",
            "Epoch 613/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2615 - val_loss: 0.2593\n",
            "Epoch 614/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2614 - val_loss: 0.2591\n",
            "Epoch 615/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2613 - val_loss: 0.2588\n",
            "Epoch 616/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2611 - val_loss: 0.2586\n",
            "Epoch 617/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2610 - val_loss: 0.2584\n",
            "Epoch 618/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2609 - val_loss: 0.2584\n",
            "Epoch 619/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2608 - val_loss: 0.2583\n",
            "Epoch 620/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2607 - val_loss: 0.2582\n",
            "Epoch 621/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2605 - val_loss: 0.2580\n",
            "Epoch 622/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2604 - val_loss: 0.2580\n",
            "Epoch 623/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2603 - val_loss: 0.2579\n",
            "Epoch 624/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2602 - val_loss: 0.2576\n",
            "Epoch 625/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2601 - val_loss: 0.2575\n",
            "Epoch 626/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2600 - val_loss: 0.2574\n",
            "Epoch 627/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2599 - val_loss: 0.2572\n",
            "Epoch 628/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2598 - val_loss: 0.2571\n",
            "Epoch 629/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2597 - val_loss: 0.2571\n",
            "Epoch 630/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2595 - val_loss: 0.2569\n",
            "Epoch 631/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2594 - val_loss: 0.2567\n",
            "Epoch 632/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2593 - val_loss: 0.2565\n",
            "Epoch 633/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2592 - val_loss: 0.2564\n",
            "Epoch 634/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2591 - val_loss: 0.2562\n",
            "Epoch 635/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.2590 - val_loss: 0.2561\n",
            "Epoch 636/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2589 - val_loss: 0.2561\n",
            "Epoch 637/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2587 - val_loss: 0.2561\n",
            "Epoch 638/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2586 - val_loss: 0.2561\n",
            "Epoch 639/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2585 - val_loss: 0.2561\n",
            "Epoch 640/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2584 - val_loss: 0.2561\n",
            "Epoch 641/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2583 - val_loss: 0.2560\n",
            "Epoch 642/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2582 - val_loss: 0.2557\n",
            "Epoch 643/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2581 - val_loss: 0.2554\n",
            "Epoch 644/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2580 - val_loss: 0.2553\n",
            "Epoch 645/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2579 - val_loss: 0.2554\n",
            "Epoch 646/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2578 - val_loss: 0.2554\n",
            "Epoch 647/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2577 - val_loss: 0.2552\n",
            "Epoch 648/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2576 - val_loss: 0.2552\n",
            "Epoch 649/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2575 - val_loss: 0.2550\n",
            "Epoch 650/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2573 - val_loss: 0.2550\n",
            "Epoch 651/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2572 - val_loss: 0.2549\n",
            "Epoch 652/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2572 - val_loss: 0.2547\n",
            "Epoch 653/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2570 - val_loss: 0.2547\n",
            "Epoch 654/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2569 - val_loss: 0.2547\n",
            "Epoch 655/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2568 - val_loss: 0.2546\n",
            "Epoch 656/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2567 - val_loss: 0.2545\n",
            "Epoch 657/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2566 - val_loss: 0.2543\n",
            "Epoch 658/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2565 - val_loss: 0.2542\n",
            "Epoch 659/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2564 - val_loss: 0.2539\n",
            "Epoch 660/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2563 - val_loss: 0.2537\n",
            "Epoch 661/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2562 - val_loss: 0.2536\n",
            "Epoch 662/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2561 - val_loss: 0.2536\n",
            "Epoch 663/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2560 - val_loss: 0.2536\n",
            "Epoch 664/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2559 - val_loss: 0.2536\n",
            "Epoch 665/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2558 - val_loss: 0.2536\n",
            "Epoch 666/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2557 - val_loss: 0.2533\n",
            "Epoch 667/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2556 - val_loss: 0.2532\n",
            "Epoch 668/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2555 - val_loss: 0.2532\n",
            "Epoch 669/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2554 - val_loss: 0.2532\n",
            "Epoch 670/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2553 - val_loss: 0.2532\n",
            "Epoch 671/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2552 - val_loss: 0.2529\n",
            "Epoch 672/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2551 - val_loss: 0.2527\n",
            "Epoch 673/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2550 - val_loss: 0.2524\n",
            "Epoch 674/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2549 - val_loss: 0.2521\n",
            "Epoch 675/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.2548 - val_loss: 0.2520\n",
            "Epoch 676/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2547 - val_loss: 0.2520\n",
            "Epoch 677/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2546 - val_loss: 0.2520\n",
            "Epoch 678/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2545 - val_loss: 0.2520\n",
            "Epoch 679/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2544 - val_loss: 0.2520\n",
            "Epoch 680/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2543 - val_loss: 0.2519\n",
            "Epoch 681/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2542 - val_loss: 0.2518\n",
            "Epoch 682/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2541 - val_loss: 0.2515\n",
            "Epoch 683/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2540 - val_loss: 0.2512\n",
            "Epoch 684/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2539 - val_loss: 0.2511\n",
            "Epoch 685/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2539 - val_loss: 0.2510\n",
            "Epoch 686/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2537 - val_loss: 0.2511\n",
            "Epoch 687/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2536 - val_loss: 0.2510\n",
            "Epoch 688/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2535 - val_loss: 0.2510\n",
            "Epoch 689/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2534 - val_loss: 0.2513\n",
            "Epoch 690/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2534 - val_loss: 0.2514\n",
            "Epoch 691/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2533 - val_loss: 0.2513\n",
            "Epoch 692/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2532 - val_loss: 0.2511\n",
            "Epoch 693/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2531 - val_loss: 0.2509\n",
            "Epoch 694/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2530 - val_loss: 0.2507\n",
            "Epoch 695/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2529 - val_loss: 0.2506\n",
            "Epoch 696/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2528 - val_loss: 0.2505\n",
            "Epoch 697/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2527 - val_loss: 0.2505\n",
            "Epoch 698/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2526 - val_loss: 0.2502\n",
            "Epoch 699/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2526 - val_loss: 0.2499\n",
            "Epoch 700/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2524 - val_loss: 0.2500\n",
            "Epoch 701/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2523 - val_loss: 0.2500\n",
            "Epoch 702/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2523 - val_loss: 0.2500\n",
            "Epoch 703/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2522 - val_loss: 0.2500\n",
            "Epoch 704/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2521 - val_loss: 0.2500\n",
            "Epoch 705/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2520 - val_loss: 0.2501\n",
            "Epoch 706/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2520 - val_loss: 0.2501\n",
            "Epoch 707/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2518 - val_loss: 0.2498\n",
            "Epoch 708/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2517 - val_loss: 0.2494\n",
            "Epoch 709/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2516 - val_loss: 0.2490\n",
            "Epoch 710/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2516 - val_loss: 0.2489\n",
            "Epoch 711/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2515 - val_loss: 0.2489\n",
            "Epoch 712/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2514 - val_loss: 0.2489\n",
            "Epoch 713/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2513 - val_loss: 0.2489\n",
            "Epoch 714/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.2512 - val_loss: 0.2487\n",
            "Epoch 715/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2511 - val_loss: 0.2486\n",
            "Epoch 716/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2510 - val_loss: 0.2484\n",
            "Epoch 717/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2509 - val_loss: 0.2483\n",
            "Epoch 718/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2508 - val_loss: 0.2483\n",
            "Epoch 719/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2508 - val_loss: 0.2482\n",
            "Epoch 720/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2507 - val_loss: 0.2479\n",
            "Epoch 721/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2506 - val_loss: 0.2478\n",
            "Epoch 722/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2505 - val_loss: 0.2478\n",
            "Epoch 723/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2504 - val_loss: 0.2477\n",
            "Epoch 724/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2503 - val_loss: 0.2475\n",
            "Epoch 725/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2503 - val_loss: 0.2474\n",
            "Epoch 726/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2502 - val_loss: 0.2473\n",
            "Epoch 727/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2501 - val_loss: 0.2472\n",
            "Epoch 728/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2500 - val_loss: 0.2474\n",
            "Epoch 729/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2499 - val_loss: 0.2474\n",
            "Epoch 730/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2498 - val_loss: 0.2474\n",
            "Epoch 731/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2498 - val_loss: 0.2476\n",
            "Epoch 732/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2497 - val_loss: 0.2477\n",
            "Epoch 733/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2496 - val_loss: 0.2475\n",
            "Epoch 734/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2495 - val_loss: 0.2475\n",
            "Epoch 735/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2495 - val_loss: 0.2475\n",
            "Epoch 736/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2494 - val_loss: 0.2473\n",
            "Epoch 737/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2493 - val_loss: 0.2472\n",
            "Epoch 738/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2492 - val_loss: 0.2472\n",
            "Epoch 739/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2491 - val_loss: 0.2470\n",
            "Epoch 740/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2490 - val_loss: 0.2468\n",
            "Epoch 741/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2489 - val_loss: 0.2465\n",
            "Epoch 742/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2488 - val_loss: 0.2462\n",
            "Epoch 743/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2488 - val_loss: 0.2461\n",
            "Epoch 744/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2487 - val_loss: 0.2460\n",
            "Epoch 745/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2486 - val_loss: 0.2461\n",
            "Epoch 746/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2485 - val_loss: 0.2464\n",
            "Epoch 747/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2484 - val_loss: 0.2465\n",
            "Epoch 748/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2484 - val_loss: 0.2464\n",
            "Epoch 749/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2483 - val_loss: 0.2462\n",
            "Epoch 750/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2482 - val_loss: 0.2462\n",
            "Epoch 751/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2481 - val_loss: 0.2462\n",
            "Epoch 752/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2481 - val_loss: 0.2461\n",
            "Epoch 753/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2480 - val_loss: 0.2460\n",
            "Epoch 754/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2479 - val_loss: 0.2458\n",
            "Epoch 755/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2478 - val_loss: 0.2458\n",
            "Epoch 756/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2477 - val_loss: 0.2458\n",
            "Epoch 757/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2477 - val_loss: 0.2456\n",
            "Epoch 758/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2476 - val_loss: 0.2455\n",
            "Epoch 759/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2475 - val_loss: 0.2453\n",
            "Epoch 760/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2475 - val_loss: 0.2452\n",
            "Epoch 761/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2473 - val_loss: 0.2452\n",
            "Epoch 762/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2472 - val_loss: 0.2453\n",
            "Epoch 763/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2472 - val_loss: 0.2453\n",
            "Epoch 764/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.2471 - val_loss: 0.2453\n",
            "Epoch 765/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2471 - val_loss: 0.2451\n",
            "Epoch 766/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2470 - val_loss: 0.2449\n",
            "Epoch 767/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2469 - val_loss: 0.2446\n",
            "Epoch 768/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2468 - val_loss: 0.2444\n",
            "Epoch 769/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2467 - val_loss: 0.2442\n",
            "Epoch 770/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2467 - val_loss: 0.2441\n",
            "Epoch 771/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2466 - val_loss: 0.2442\n",
            "Epoch 772/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2465 - val_loss: 0.2442\n",
            "Epoch 773/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2464 - val_loss: 0.2440\n",
            "Epoch 774/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2463 - val_loss: 0.2439\n",
            "Epoch 775/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2463 - val_loss: 0.2437\n",
            "Epoch 776/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2462 - val_loss: 0.2435\n",
            "Epoch 777/1000\n",
            "2506/2506 [==============================] - 0s 16us/sample - loss: 0.2462 - val_loss: 0.2433\n",
            "Epoch 778/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2461 - val_loss: 0.2434\n",
            "Epoch 779/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2460 - val_loss: 0.2432\n",
            "Epoch 780/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2460 - val_loss: 0.2433\n",
            "Epoch 781/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2458 - val_loss: 0.2432\n",
            "Epoch 782/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2458 - val_loss: 0.2430\n",
            "Epoch 783/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2457 - val_loss: 0.2429\n",
            "Epoch 784/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2457 - val_loss: 0.2428\n",
            "Epoch 785/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2456 - val_loss: 0.2428\n",
            "Epoch 786/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2455 - val_loss: 0.2429\n",
            "Epoch 787/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2454 - val_loss: 0.2429\n",
            "Epoch 788/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2453 - val_loss: 0.2430\n",
            "Epoch 789/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2452 - val_loss: 0.2429\n",
            "Epoch 790/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2452 - val_loss: 0.2429\n",
            "Epoch 791/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2451 - val_loss: 0.2426\n",
            "Epoch 792/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2451 - val_loss: 0.2425\n",
            "Epoch 793/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.2450 - val_loss: 0.2425\n",
            "Epoch 794/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2449 - val_loss: 0.2426\n",
            "Epoch 795/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2448 - val_loss: 0.2427\n",
            "Epoch 796/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2447 - val_loss: 0.2428\n",
            "Epoch 797/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2447 - val_loss: 0.2428\n",
            "Epoch 798/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2446 - val_loss: 0.2426\n",
            "Epoch 799/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2445 - val_loss: 0.2423\n",
            "Epoch 800/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2444 - val_loss: 0.2421\n",
            "Epoch 801/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2444 - val_loss: 0.2421\n",
            "Epoch 802/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2443 - val_loss: 0.2422\n",
            "Epoch 803/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2442 - val_loss: 0.2421\n",
            "Epoch 804/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2442 - val_loss: 0.2419\n",
            "Epoch 805/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2442 - val_loss: 0.2415\n",
            "Epoch 806/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2441 - val_loss: 0.2415\n",
            "Epoch 807/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2440 - val_loss: 0.2415\n",
            "Epoch 808/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2439 - val_loss: 0.2416\n",
            "Epoch 809/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2439 - val_loss: 0.2418\n",
            "Epoch 810/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2438 - val_loss: 0.2416\n",
            "Epoch 811/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2437 - val_loss: 0.2415\n",
            "Epoch 812/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2436 - val_loss: 0.2413\n",
            "Epoch 813/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2435 - val_loss: 0.2411\n",
            "Epoch 814/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2435 - val_loss: 0.2409\n",
            "Epoch 815/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2434 - val_loss: 0.2409\n",
            "Epoch 816/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2433 - val_loss: 0.2410\n",
            "Epoch 817/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2433 - val_loss: 0.2411\n",
            "Epoch 818/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2432 - val_loss: 0.2412\n",
            "Epoch 819/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2431 - val_loss: 0.2410\n",
            "Epoch 820/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2431 - val_loss: 0.2409\n",
            "Epoch 821/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2430 - val_loss: 0.2408\n",
            "Epoch 822/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2429 - val_loss: 0.2407\n",
            "Epoch 823/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2428 - val_loss: 0.2408\n",
            "Epoch 824/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2428 - val_loss: 0.2409\n",
            "Epoch 825/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2428 - val_loss: 0.2406\n",
            "Epoch 826/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2427 - val_loss: 0.2407\n",
            "Epoch 827/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2426 - val_loss: 0.2406\n",
            "Epoch 828/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2425 - val_loss: 0.2401\n",
            "Epoch 829/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2425 - val_loss: 0.2398\n",
            "Epoch 830/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2424 - val_loss: 0.2397\n",
            "Epoch 831/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2424 - val_loss: 0.2396\n",
            "Epoch 832/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2423 - val_loss: 0.2397\n",
            "Epoch 833/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2422 - val_loss: 0.2397\n",
            "Epoch 834/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.2421 - val_loss: 0.2399\n",
            "Epoch 835/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2421 - val_loss: 0.2398\n",
            "Epoch 836/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2420 - val_loss: 0.2399\n",
            "Epoch 837/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2419 - val_loss: 0.2398\n",
            "Epoch 838/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2419 - val_loss: 0.2397\n",
            "Epoch 839/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2418 - val_loss: 0.2396\n",
            "Epoch 840/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2417 - val_loss: 0.2398\n",
            "Epoch 841/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2417 - val_loss: 0.2397\n",
            "Epoch 842/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2416 - val_loss: 0.2395\n",
            "Epoch 843/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2415 - val_loss: 0.2394\n",
            "Epoch 844/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2415 - val_loss: 0.2391\n",
            "Epoch 845/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2414 - val_loss: 0.2390\n",
            "Epoch 846/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2413 - val_loss: 0.2390\n",
            "Epoch 847/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2413 - val_loss: 0.2390\n",
            "Epoch 848/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2412 - val_loss: 0.2391\n",
            "Epoch 849/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2411 - val_loss: 0.2391\n",
            "Epoch 850/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2411 - val_loss: 0.2389\n",
            "Epoch 851/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2410 - val_loss: 0.2391\n",
            "Epoch 852/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2410 - val_loss: 0.2392\n",
            "Epoch 853/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2409 - val_loss: 0.2390\n",
            "Epoch 854/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2408 - val_loss: 0.2388\n",
            "Epoch 855/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2407 - val_loss: 0.2385\n",
            "Epoch 856/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2407 - val_loss: 0.2384\n",
            "Epoch 857/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2406 - val_loss: 0.2384\n",
            "Epoch 858/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2406 - val_loss: 0.2385\n",
            "Epoch 859/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2405 - val_loss: 0.2385\n",
            "Epoch 860/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2404 - val_loss: 0.2385\n",
            "Epoch 861/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2404 - val_loss: 0.2383\n",
            "Epoch 862/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2403 - val_loss: 0.2381\n",
            "Epoch 863/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2402 - val_loss: 0.2381\n",
            "Epoch 864/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2402 - val_loss: 0.2381\n",
            "Epoch 865/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2401 - val_loss: 0.2380\n",
            "Epoch 866/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2401 - val_loss: 0.2379\n",
            "Epoch 867/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2400 - val_loss: 0.2377\n",
            "Epoch 868/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2399 - val_loss: 0.2374\n",
            "Epoch 869/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2399 - val_loss: 0.2373\n",
            "Epoch 870/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2399 - val_loss: 0.2371\n",
            "Epoch 871/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2398 - val_loss: 0.2371\n",
            "Epoch 872/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2398 - val_loss: 0.2371\n",
            "Epoch 873/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2397 - val_loss: 0.2373\n",
            "Epoch 874/1000\n",
            "2506/2506 [==============================] - 0s 13us/sample - loss: 0.2396 - val_loss: 0.2374\n",
            "Epoch 875/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2396 - val_loss: 0.2376\n",
            "Epoch 876/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2395 - val_loss: 0.2376\n",
            "Epoch 877/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2395 - val_loss: 0.2377\n",
            "Epoch 878/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2394 - val_loss: 0.2376\n",
            "Epoch 879/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2393 - val_loss: 0.2375\n",
            "Epoch 880/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2392 - val_loss: 0.2374\n",
            "Epoch 881/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2392 - val_loss: 0.2372\n",
            "Epoch 882/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2391 - val_loss: 0.2371\n",
            "Epoch 883/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2390 - val_loss: 0.2372\n",
            "Epoch 884/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2390 - val_loss: 0.2376\n",
            "Epoch 885/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2390 - val_loss: 0.2375\n",
            "Epoch 886/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2389 - val_loss: 0.2373\n",
            "Epoch 887/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2388 - val_loss: 0.2369\n",
            "Epoch 888/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2388 - val_loss: 0.2365\n",
            "Epoch 889/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2387 - val_loss: 0.2365\n",
            "Epoch 890/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2386 - val_loss: 0.2365\n",
            "Epoch 891/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2386 - val_loss: 0.2364\n",
            "Epoch 892/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2385 - val_loss: 0.2363\n",
            "Epoch 893/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2385 - val_loss: 0.2360\n",
            "Epoch 894/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2384 - val_loss: 0.2359\n",
            "Epoch 895/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2384 - val_loss: 0.2358\n",
            "Epoch 896/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2383 - val_loss: 0.2358\n",
            "Epoch 897/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2382 - val_loss: 0.2360\n",
            "Epoch 898/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2381 - val_loss: 0.2361\n",
            "Epoch 899/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2381 - val_loss: 0.2363\n",
            "Epoch 900/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2380 - val_loss: 0.2362\n",
            "Epoch 901/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2380 - val_loss: 0.2360\n",
            "Epoch 902/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2379 - val_loss: 0.2359\n",
            "Epoch 903/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2379 - val_loss: 0.2358\n",
            "Epoch 904/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2378 - val_loss: 0.2358\n",
            "Epoch 905/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2377 - val_loss: 0.2358\n",
            "Epoch 906/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2377 - val_loss: 0.2359\n",
            "Epoch 907/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2377 - val_loss: 0.2359\n",
            "Epoch 908/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2376 - val_loss: 0.2357\n",
            "Epoch 909/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2375 - val_loss: 0.2357\n",
            "Epoch 910/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2375 - val_loss: 0.2355\n",
            "Epoch 911/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2374 - val_loss: 0.2355\n",
            "Epoch 912/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2374 - val_loss: 0.2356\n",
            "Epoch 913/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2373 - val_loss: 0.2356\n",
            "Epoch 914/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.2373 - val_loss: 0.2355\n",
            "Epoch 915/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2372 - val_loss: 0.2353\n",
            "Epoch 916/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2371 - val_loss: 0.2351\n",
            "Epoch 917/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2371 - val_loss: 0.2349\n",
            "Epoch 918/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2370 - val_loss: 0.2349\n",
            "Epoch 919/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2369 - val_loss: 0.2349\n",
            "Epoch 920/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2369 - val_loss: 0.2346\n",
            "Epoch 921/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2369 - val_loss: 0.2345\n",
            "Epoch 922/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2368 - val_loss: 0.2347\n",
            "Epoch 923/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2367 - val_loss: 0.2348\n",
            "Epoch 924/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2367 - val_loss: 0.2346\n",
            "Epoch 925/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2366 - val_loss: 0.2347\n",
            "Epoch 926/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2366 - val_loss: 0.2345\n",
            "Epoch 927/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2365 - val_loss: 0.2342\n",
            "Epoch 928/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2365 - val_loss: 0.2340\n",
            "Epoch 929/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2364 - val_loss: 0.2339\n",
            "Epoch 930/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2364 - val_loss: 0.2339\n",
            "Epoch 931/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2363 - val_loss: 0.2339\n",
            "Epoch 932/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2362 - val_loss: 0.2339\n",
            "Epoch 933/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2362 - val_loss: 0.2339\n",
            "Epoch 934/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2361 - val_loss: 0.2339\n",
            "Epoch 935/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2360 - val_loss: 0.2341\n",
            "Epoch 936/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2360 - val_loss: 0.2343\n",
            "Epoch 937/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2360 - val_loss: 0.2343\n",
            "Epoch 938/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2359 - val_loss: 0.2342\n",
            "Epoch 939/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2359 - val_loss: 0.2340\n",
            "Epoch 940/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2358 - val_loss: 0.2340\n",
            "Epoch 941/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2357 - val_loss: 0.2340\n",
            "Epoch 942/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2357 - val_loss: 0.2337\n",
            "Epoch 943/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2356 - val_loss: 0.2335\n",
            "Epoch 944/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2356 - val_loss: 0.2331\n",
            "Epoch 945/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2355 - val_loss: 0.2330\n",
            "Epoch 946/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2356 - val_loss: 0.2330\n",
            "Epoch 947/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2355 - val_loss: 0.2331\n",
            "Epoch 948/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2354 - val_loss: 0.2332\n",
            "Epoch 949/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2353 - val_loss: 0.2333\n",
            "Epoch 950/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2353 - val_loss: 0.2334\n",
            "Epoch 951/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2352 - val_loss: 0.2333\n",
            "Epoch 952/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2352 - val_loss: 0.2332\n",
            "Epoch 953/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2351 - val_loss: 0.2329\n",
            "Epoch 954/1000\n",
            "2506/2506 [==============================] - 0s 12us/sample - loss: 0.2350 - val_loss: 0.2329\n",
            "Epoch 955/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2350 - val_loss: 0.2330\n",
            "Epoch 956/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2349 - val_loss: 0.2331\n",
            "Epoch 957/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2349 - val_loss: 0.2333\n",
            "Epoch 958/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2349 - val_loss: 0.2335\n",
            "Epoch 959/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2348 - val_loss: 0.2335\n",
            "Epoch 960/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2348 - val_loss: 0.2334\n",
            "Epoch 961/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2347 - val_loss: 0.2331\n",
            "Epoch 962/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2346 - val_loss: 0.2327\n",
            "Epoch 963/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2345 - val_loss: 0.2323\n",
            "Epoch 964/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2345 - val_loss: 0.2321\n",
            "Epoch 965/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2345 - val_loss: 0.2319\n",
            "Epoch 966/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2345 - val_loss: 0.2320\n",
            "Epoch 967/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2344 - val_loss: 0.2321\n",
            "Epoch 968/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2343 - val_loss: 0.2319\n",
            "Epoch 969/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2343 - val_loss: 0.2319\n",
            "Epoch 970/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2343 - val_loss: 0.2324\n",
            "Epoch 971/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2342 - val_loss: 0.2327\n",
            "Epoch 972/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2341 - val_loss: 0.2326\n",
            "Epoch 973/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2341 - val_loss: 0.2326\n",
            "Epoch 974/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2340 - val_loss: 0.2322\n",
            "Epoch 975/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2340 - val_loss: 0.2319\n",
            "Epoch 976/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2339 - val_loss: 0.2319\n",
            "Epoch 977/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2338 - val_loss: 0.2320\n",
            "Epoch 978/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2338 - val_loss: 0.2318\n",
            "Epoch 979/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2337 - val_loss: 0.2316\n",
            "Epoch 980/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2337 - val_loss: 0.2316\n",
            "Epoch 981/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2336 - val_loss: 0.2316\n",
            "Epoch 982/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2336 - val_loss: 0.2317\n",
            "Epoch 983/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2335 - val_loss: 0.2317\n",
            "Epoch 984/1000\n",
            "2506/2506 [==============================] - 0s 11us/sample - loss: 0.2335 - val_loss: 0.2317\n",
            "Epoch 985/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2334 - val_loss: 0.2317\n",
            "Epoch 986/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2334 - val_loss: 0.2317\n",
            "Epoch 987/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2333 - val_loss: 0.2316\n",
            "Epoch 988/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2333 - val_loss: 0.2313\n",
            "Epoch 989/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2332 - val_loss: 0.2310\n",
            "Epoch 990/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2332 - val_loss: 0.2307\n",
            "Epoch 991/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2333 - val_loss: 0.2307\n",
            "Epoch 992/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2332 - val_loss: 0.2310\n",
            "Epoch 993/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2330 - val_loss: 0.2313\n",
            "Epoch 994/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2330 - val_loss: 0.2315\n",
            "Epoch 995/1000\n",
            "2506/2506 [==============================] - 0s 14us/sample - loss: 0.2330 - val_loss: 0.2317\n",
            "Epoch 996/1000\n",
            "2506/2506 [==============================] - 0s 9us/sample - loss: 0.2330 - val_loss: 0.2317\n",
            "Epoch 997/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2329 - val_loss: 0.2316\n",
            "Epoch 998/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2329 - val_loss: 0.2312\n",
            "Epoch 999/1000\n",
            "2506/2506 [==============================] - 0s 8us/sample - loss: 0.2328 - val_loss: 0.2310\n",
            "Epoch 1000/1000\n",
            "2506/2506 [==============================] - 0s 10us/sample - loss: 0.2327 - val_loss: 0.2307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CebcKzSW_g6P"
      },
      "source": [
        "Let's visualize the training and validation losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U9G1OHXoroBs",
        "outputId": "46f5855b-e847-4653-f39f-0fdb01078a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "for label in [\"loss\",\"val_loss\"]:\n",
        "    plt.plot(train_history.history[label], label=label)\n",
        "\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title(\"The final validation loss: {}\".format(train_history.history[\"val_loss\"][-1]))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAFNCAYAAAC5cXZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREdiCFPRERERERkR3o/wOPKyXvUPhxnQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rs_-XnYhKl_N"
      },
      "source": [
        "Finally, let's look at the parameters for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q4gtwBT7Kgh0",
        "outputId": "870bfdca-669f-401f-fa18-ff9403998d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for layer in model.layers:\n",
        "  print(\"{}, {}\".format(layer.name, layer.get_weights()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_layer, [array([[0.2797624]], dtype=float32), array([[-3.6514106]], dtype=float32), array([0.00288544], dtype=float32)]\n",
            "output_layer, [array([[-1.102846]], dtype=float32), array([-0.00604694], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bqFBu_dCsUqi"
      },
      "source": [
        "**QUESTION**: \n",
        "* Relate the above weights to the terms in the equation for the vanilla RNN we saw earlier, namely:\n",
        "  * input-to-hidden $W_{xh}$,\n",
        "  * hidden-to-hidden $W_{hh}$,\n",
        "  * hidden-to-output weights $W_{hy}$\n",
        "  * recurrent and out biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0FHaN-VXfxEl"
      },
      "source": [
        "###Make predictions using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IQl_msx-4o3E",
        "outputId": "4fef40b0-49a7-4f97-82b9-2a22f35bf598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "y_pred = model.predict(X_test[:100])\n",
        "plt.figure(figsize=(19,3))\n",
        "\n",
        "plt.plot(y_test[:100], label=\"true\")\n",
        "plt.plot(y_pred, label=\"predicted\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABFUAAADFCAYAAACVSs2bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEKIayBJFSGEEEIIIYQQQohr8P/tuapSkTsWlQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1368x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MQj5P-FKOgqG"
      },
      "source": [
        "**YOUR TASKS**: \n",
        "* [**ALL**] Change the learning rate up to 5 orders of magnitude larger and smaller and retrain. What happens when it is too large? What happens when it is too small?\n",
        "* [**ALL**] Change the `SimpleRNN` to `GRU`.\n",
        "  * You can read more about LSTMs in [this blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah. \n",
        "  * What is the effect on the number of parameters? Can you explain why? Now do the same for `LSTM`.\n",
        "* [**INTERMEDIATE**] Note that the loss does not decrease much after around epoch 400. Add \"Early Stopping with patience\" to the `model.fit()` function to stop it from training beyond this point. **Hint**: Look at tf.keras.callbacks.\n",
        "  * *Early stopping* is a technique where we stop training the model once it's performance on validation data stops improving, and is a very common approach to prevent overfitting. Early stopping *with patience* means as soon as the model starts doing worse on validation we wait for at least `patience` more evaluations before stopping training, and if it improves within that time, we reset the counter. The patience parameter is a way to ensure we don't stop by accident, as the validation loss can fluctuate randomly from epoch to epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_totIpxmZ8_v"
      },
      "source": [
        "##Generating Shakespeare\n",
        "\n",
        "Now let's build an RNN language model to generate Shakespearian English! A language model is trained to assign high probabilities to sequences of words or sentences that are well formed, and low probabilities to sequences which are not realistic. When the model is trained, one can use it to *generate* data that is similar to the training data.\n",
        "\n",
        "Our data is now sequences of discrete symbols (characters). But neural networks operate in continuous spaces, and so we need to take the discrete language data, and **embed** it in a continuous space. To do this, we'll simply break up the data into sequences of characters, and represent each character using a learned vector. This is a standard trick for processing text using neural networks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dFQtnDLZa3c"
      },
      "source": [
        "### Download and Preprocess the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tmmjc-EigwCw"
      },
      "source": [
        "We first download the data and examine what it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hybIopOLPD4f",
        "outputId": "5802b580-8528-4749-930c-43500fff618f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "context = ssl._create_unverified_context()\n",
        "shakespeare_url = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\n",
        "\n",
        "data = urllib.request.urlopen(shakespeare_url, context=context)\n",
        "all_text = data.read().lower().decode('utf8')\n",
        "\n",
        "print(\"Downloaded Shakespeare data with {} characters.\".format(len(all_text)))\n",
        "print(\"FIRST 1000 CHARACTERS: \")\n",
        "print(all_text[:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded Shakespeare data with 4573338 characters.\n",
            "FIRST 1000 CHARACTERS: \n",
            "first citizen:\n",
            "before we proceed any further, hear me speak.\n",
            "\n",
            "all:\n",
            "speak, speak.\n",
            "\n",
            "first citizen:\n",
            "you are all resolved rather to die than to famish?\n",
            "\n",
            "all:\n",
            "resolved. resolved.\n",
            "\n",
            "first citizen:\n",
            "first, you know caius marcius is chief enemy to the people.\n",
            "\n",
            "all:\n",
            "we know't, we know't.\n",
            "\n",
            "first citizen:\n",
            "let us kill him, and we'll have corn at our own price.\n",
            "is't a verdict?\n",
            "\n",
            "all:\n",
            "no more talking on't; let it be done: away, away!\n",
            "\n",
            "second citizen:\n",
            "one word, good citizens.\n",
            "\n",
            "first citizen:\n",
            "we are accounted poor citizens, the patricians good.\n",
            "what authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know i\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jIuagNcdQLqM",
        "colab": {}
      },
      "source": [
        "training_text = all_text[:1000000] # Keep only the first 1 million characters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UK1x69AngvLJ"
      },
      "source": [
        "We now preprocess the text data as follows:\n",
        "\n",
        "1. Extract the vocabulary of all `vocab_size` unique characters appearing in the data.\n",
        "\n",
        "2. Assign each character a unique integer id in `0 <= id < vocab_size`. This is so we can map the characters to unique embedding vectors. This is a common way to map discrete inputs to continuous vectors that neural networks can work with. See e.g. this [blog post](https://www.tensorflow.org/tutorials/representation/word2vec), or [this one](http://ruder.io/word-embeddings-1/) for more information.\n",
        "\n",
        "3. Split the data into sequences (\"windows\") of `max_len` characters (the input to the model) followed by the next character as target. E.g. using `max_len=5` the sentence \"I saw a cat\" (11 characters) will get split into \"I saw\" and /space/, \"/space/saw/space/\" and \"a\", \"saw a\" and /space/, etc. To add some variation, we skip `step` characters between each sequence (i.e. we use a \"sliding window of `max_len` with stride `step`\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zPvsNXKZPJKz",
        "outputId": "292fd27f-f056-44d5-8d88-911bcacdcb40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "max_len = 30  # We only consider this many previous data points (characters)\n",
        "step = 3 # We start a new training sequence every `step` characters\n",
        "sentences = [] # This holds our extracted sequences\n",
        "next_chars = [] # This holds the targets (the follow-up characters)\n",
        "\n",
        "chars = sorted(list(set(training_text)))  # List of unique characters in the corpus\n",
        "vocab_size = len(chars)\n",
        "print('Number of unique characters: ', vocab_size)\n",
        "print(chars)\n",
        "\n",
        "# Construct dictionaries mapping unique characters to their index in `chars` and reverse\n",
        "char2index = dict((c, chars.index(c)) for c in chars)\n",
        "index2char = dict((chars.index(c), c) for c in chars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique characters:  39\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O2PhcQwrc2JX"
      },
      "source": [
        "Now we encode the training data by mapping each character to its unique integer id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xY0qXZmVq8kW",
        "outputId": "9b988379-f400-462f-adc8-920bbcf5464d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(0, len(training_text) - max_len, step):\n",
        "    sentences.append([char2index[s] for s in training_text[i: i + max_len]])\n",
        "    next_chars.append([char2index[training_text[i + max_len]]])\n",
        "\n",
        "print('Number of extracted sequences:', len(sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of extracted sequences: 333324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PsZy6Kshc-Sp"
      },
      "source": [
        "This yields the following numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhgSaP5ntDtq",
        "outputId": "546e84eb-9a6f-4b1c-96fc-8eb7ac243e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X, Y = np.array(sentences, dtype=np.int64), np.array(next_chars, dtype=np.int64)\n",
        "X.shape, Y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((333324, 30), (333324, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sBR2LsXjmqTU"
      },
      "source": [
        "Let's take a look at the first example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2H1jobcrmwK7",
        "outputId": "1255c899-10ad-4867-9caf-2c2c1c55510f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(\"X[0].shape = {}, Y[0].shape = {}\".format(X[0].shape, Y[0].shape))\n",
        "print(\"X[0]: \", X[0])\n",
        "print(\"Y[0]: \", Y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X[0].shape = (30,), Y[0].shape = (1,)\n",
            "X[0]:  [18 21 30 31 32  1 15 21 32 21 38 17 26 10  0 14 17 18 27 30 17  1 35 17\n",
            "  1 28 30 27 15 17]\n",
            "Y[0]:  [17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tEQWEZlSZgpF"
      },
      "source": [
        "###Build an RNN language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rCysOLWadPHF"
      },
      "source": [
        "A **language model** estimates a probability distribution over sequences $\\mathbb{x}_{1:N} = (x_1, x_2, ..., x_N)$ by breaking up the full joint probability into a sequence of conditional probabilities using the **[chain-rule of probability](https://en.wikipedia.org/wiki/Chain_rule_(probability))**:\n",
        "\n",
        "\\begin{align}\n",
        " p(\\mathbb{x}_{1:N}) &= p(x_1) \\cdot p(x_2 | x_1) \\cdot p(x_3 | x_2, x_1) \\ldots \\\\\n",
        " &= \\Pi_1^N p(x_i | \\mathbb{x}_{1:i-1})\n",
        "\\end{align}\n",
        "\n",
        "In other words, to model the probability of the phrase \"*i saw a cat*\" at the character level, the model learns to estimate the probabilities for p(i), p(/space/| i), p(c | i, /space/), and so forth, and multiplies them together. \n",
        "\n",
        "There are many different ways in which to estimate these individual probabilities. But one particularly effective way is to use an RNN! To do this, we'll therefore be modeling the  $p(x_i | \\mathbb{x}_{1:i-1})$ terms using an RNN conditioned on $\\mathbb{x}_{1:i-1}$.\n",
        "\n",
        "* We model these probabilities at the character-level, so we'll use an `Embedding` layer as the first layer of our model to map the discrete character id's to real-valued embedding vectors. \n",
        "* Next, the RNN-core will map these sequences of character embeddings to a probability distribution over all characters $p(x_i | \\mathbb{x}_{1:i-1}) \\in \\mathbb{R}^\\textrm{vocab_size}$ at every step of the sequence. To do this, the RNN will map the embeddings to a sequence of *hidden states*. We will then use a `Dense` layer to map from the RNN hidden state to an output distribution over the total number of characters using a [`softmax`](https://en.wikipedia.org/wiki/Softmax_function) activation.\n",
        "\n",
        "We can do this with a few lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k8AxhQuePOCN",
        "outputId": "86c081b6-0231-47bc-a9cb-342758501c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "embedding_dim = 32   # Map each character to a unique vector of this dimension\n",
        "vocab_size = len(chars)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(\n",
        "  vocab_size, embedding_dim,\n",
        "  input_length=max_len,\n",
        "  embeddings_initializer=tf.keras.initializers.TruncatedNormal))\n",
        "model.add(tf.keras.layers.LSTM(\n",
        "  128,\n",
        "  input_shape=(max_len, embedding_dim),  # NB: Ensure this matches the embedding_dim!\n",
        "  dropout=0.1,  # input-to-hidden drop-probability\n",
        "  recurrent_dropout=0.2))  # hidden-to-hidden drop-probability\n",
        "model.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0708 18:30:15.728784 139985615468416 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:94: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 32)            1248      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               82432     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 39)                5031      \n",
            "=================================================================\n",
            "Total params: 88,711\n",
            "Trainable params: 88,711\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m4fFjFtZokf2"
      },
      "source": [
        "###Select the optimizer and loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RHOWm55whZUt"
      },
      "source": [
        "Once we have a model that can map sequence of characters to a probability distribution over the next character in the sequence, we can train it using **maximum likelihood** on the training set to find the model parameters which maximizes the probability of the training data. Again, this is very simple to do by choosing an optimizer and selecting the `sparse_categorical_crossentropy` loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hKJNZXs7PSW-",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss='sparse_categorical_crossentropy'\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dEr9hEqFiGx6"
      },
      "source": [
        "###Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3zXa95pBPUw3",
        "colab": {}
      },
      "source": [
        "def sample_with_temp(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AyH6U3er5fis",
        "outputId": "15af9b6d-694d-40d0-f373-822b93a4e880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def shift_and_append(test_arr, next_item):\n",
        "  '''Returns a copy of test_arr with items shifted one position to the left and\n",
        "     next_item appended.\n",
        "  '''\n",
        "  tmp = np.empty_like(test_arr)\n",
        "  tmp[:,:-1] = test_arr[:,1:]\n",
        "  tmp[:,-1] = next_item\n",
        "  return tmp\n",
        "\n",
        "## TEST the above function:\n",
        "test_arr = np.array([[1,2,3,4]])\n",
        "\n",
        "print(\"test_arr = {}\".format(test_arr))\n",
        "test_arr = shift_and_append(test_arr, 5)\n",
        "print(\"roll_arr(test_arr, 5) = {}\".format(test_arr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_arr = [[1 2 3 4]]\n",
            "roll_arr(test_arr, 5) = [[2 3 4 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q9le2p0YxeYD",
        "colab": {}
      },
      "source": [
        "def sample_from_model(model,\n",
        "                      num_generate=400,\n",
        "                      prev_text=None,  # the text used to condition the model\n",
        "                      temperatures=[0.2, 0.5, 1.0, 1.2]):\n",
        "\n",
        "    if not prev_text:\n",
        "      # Select a text seed at random\n",
        "      start_index = random.randint(0, len(training_text) - max_len - 1)\n",
        "      while ((start_index < (len(training_text) - max_len - 1)) and (\n",
        "        training_text[start_index - 1] is not ' ')):\n",
        "        start_index += 1  # Advance to beginning of new word\n",
        "      prev_text = training_text[start_index: start_index + max_len]\n",
        "\n",
        "    if len(prev_text) != max_len:\n",
        "      print(\"`prev_text` must be of length `max_len`.\")\n",
        "      return\n",
        "\n",
        "    print('GENERATING TEXT WITH SEED: \\n\"' + prev_text + '\"')\n",
        "    prev_text_arr = np.array(\n",
        "      [[char2index[c] for c in prev_text]], dtype=np.int64)\n",
        "\n",
        "    for temp in temperatures:\n",
        "        print('==TEMPERATURE:', temp)\n",
        "        sys.stdout.write(prev_text)\n",
        "\n",
        "        # Start with the same sampled text for all temperatures\n",
        "        generated_text = prev_text\n",
        "        generated_text_arr = prev_text_arr\n",
        "\n",
        "        # Now generate this many characters\n",
        "        for i in range(num_generate):\n",
        "\n",
        "            # Get the output softmax given the conditioning text\n",
        "            #prev_text = generated_text_enc[np.newaxis,:]\n",
        "            preds = model.predict(generated_text_arr, verbose=0)[0]\n",
        "\n",
        "            next_index = sample_with_temp(preds, temp)\n",
        "            next_char = index2char[next_index]\n",
        "            generated_text += next_char\n",
        "            generated_text = generated_text[1:]\n",
        "\n",
        "            # Left-shift and add into encoded array\n",
        "            generated_text_arr = shift_and_append(generated_text_arr, next_index)\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a78CrsaMiKrA"
      },
      "source": [
        "###Train the model\n",
        "\n",
        "Let's train the model! The code below will train the model on a subset of the available data, and then generate from the model every `sample_every` number of batches.\n",
        "\n",
        "To generate from the model, we use `model.predict()` on a sequence of `max_len` conditioning characters to produce an output distribution over `vocab_size` characters. We then sample one character from this distribution and shift everything up by one and append the new characters. By repeating this, we can generate text from the (partially-trained) model.\n",
        "\n",
        "**NOTE**: \n",
        "* It takes a while to train a model that starts generating anything resembling the Shapespeare text! In general it should start getting the rough structure in place around the 100K training example mark (examples, not batches). But to generate any meaningful words will need several hundred thousand examples.\n",
        "* We sample with *temperature*. This is a way to sharpen or flatten the probabilities produced by the model. By lowering the temperature, we emphasize the modes of the predicted distribution, and by increasing the temperature, we flatten the modes (tends towards uniform). Higher temperatures therefore encourage the model to be more 'creative', instead of always choosing the most likely next character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFaGh-Yusm4x",
        "colab_type": "code",
        "outputId": "05ac4d97-9163-4412-9d37-a44e03eb17ba",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 48
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-85bb6fdc-f708-4e58-9b59-84454b6b983c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-85bb6fdc-f708-4e58-9b59-84454b6b983c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RcFKgkdPXuI",
        "outputId": "7462c7c3-8d9a-41a2-9186-7cd211cd7676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 128\n",
        "total_num_batches = X.shape[0] // batch_size\n",
        "sample_every = 256  # Train on this many batches, then generate something\n",
        "\n",
        "print(\"Training on {} batches in total.\".format(total_num_batches))\n",
        "\n",
        "for cur_batch in range(0, total_num_batches, sample_every):\n",
        "    print('TRAINING ON BATCH {} to {} (example {} to {})'.format(\n",
        "      cur_batch, cur_batch + sample_every,\n",
        "      cur_batch * batch_size, (cur_batch + sample_every) * batch_size)\n",
        "    )\n",
        "\n",
        "    X_batch = X[batch_size * cur_batch : batch_size * (cur_batch + sample_every), :]\n",
        "    Y_batch = Y[batch_size * cur_batch : batch_size * (cur_batch + sample_every), :]\n",
        "\n",
        "    '''\n",
        "    # Show the first 5 examples to make sure we're not training on garbage\n",
        "    print(\"X_batch.shape = {}\".format(X_batch.shape))\n",
        "    print(\"Y_batch.shape = {}\".format(Y_batch.shape))\n",
        "    print(\"FIRST 5 EXAMPLES:\")\n",
        "    for num in range(5):\n",
        "      in_seq = [index2char[int(indx)] for indx in np.nditer(X_batch[num, :])]\n",
        "      next_char = index2char[Y_batch[num, 0]]\n",
        "      print(str(num) + '. ' + ''.join(in_seq) + '-->' + next_char)\n",
        "    '''\n",
        "\n",
        "    model.fit(X_batch, Y_batch,\n",
        "              batch_size=batch_size,\n",
        "              epochs=1,\n",
        "              verbose=1)\n",
        "\n",
        "    print(\"GENERATING SOME RANDOM TEXT FROM THE MODEL\")\n",
        "    sample_from_model(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0708 18:30:57.057747 139985615468416 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training on 2604 batches in total.\n",
            "TRAINING ON BATCH 0 to 256 (example 0 to 32768)\n",
            "32768/32768 [==============================] - 15s 464us/sample - loss: 2.9973\n",
            "GENERATING SOME RANDOM TEXT FROM THE MODEL\n",
            "GENERATING TEXT WITH SEED: \n",
            "\"have poison'd good camillo's h\"\n",
            "==TEMPERATURE: 0.2\n",
            "have poison'd good camillo's he he he hous ore thon ar he hone ho hor houn hous houn the the he he con hour hothe he athe hon woo hor hor ho he the houn he he hothe he he hathe he ire in the the hous the ho hor athe hothe he he as hon hor he he the he he the hitar hon the the ho he hor hore hon are he hore an an ho ar he hor he hor are he he he an he ano hothe he ar hor he he ho he he the ire he hore as hor he he he the tor he\n",
            "==TEMPERATURE: 0.5\n",
            "have poison'd good camillo's hol ho ire e toi bothan hiore weol ani ais aaf arare sod the holtoy toi he en then hicor paur chin he theit ee homis\n",
            "thon thi hon horen he hin antu uthori he he hei thoy wole the hen\n",
            " poin he thouds mord hoche hont fe or h thhouro the ittoran aore\n",
            "wonce hout moi hi or\n",
            "hos ye ol ton the ios cotli hohe an hee lio the toran atle are hed theu he yt hon oo hous sor an poyu hin the ul s wone heu ye iruun\n",
            "==TEMPERATURE: 1.0\n",
            "have poison'd good camillo's hetuiuis adtr hyifmet ro chof whoui? ok men\n",
            "pami!amtmmm; louuslm piol edgivasou\n",
            "bine\n",
            "\n",
            "iot\n",
            "s aun h've feu fo sydiuilsn taf h urnumsn\n",
            "metllthe li.ent\n",
            "anuu:hirn ciachhedd\n",
            "visif weihuften e hedsgere davo mone mood:\n",
            "aise, helinulune'rir?r na s ahoiwit dh hoire arthscou\n",
            "hvahg  hivf he whonothe pothe? hheurs d womk wio iohesud heutui\n",
            "d tyeu\n",
            "yutwowh thetd, ungamar oed ffo whcar :it hoorlwiil\n",
            "og\n",
            "fnn bcde so\n",
            "==TEMPERATURE: 1.2\n",
            "have poison'd good camillo's hetdteo hwha nis\n",
            "er\n",
            "ytlge.el,d, focldevsrtirnzops\n",
            "chotml, ictyethen we uh\n",
            "coutalotolswa\n",
            "putatcanlzeimlh omisctacwthedkares fe toeksdelsvl,i-mi fove vo gouu\n",
            "sn kinr 'iwe; velhm\n",
            "wamog'hihtve atcvoammo wgnolof hfchin .up? i iwwo 'o\n",
            "lcihiutd lounisd\n",
            "p,ord  tacsroorrhe ihis\n",
            "decmap.ni n hitheruuh!\n",
            "cthe putjpstpomoom\n",
            "shi-ab e tnrtvohima, weoyy irm horj\n",
            "limen\n",
            "mak miut hootlure-se.ur:k mof rogh\n",
            "hi tl.s,s 'r\n",
            "TRAINING ON BATCH 256 to 512 (example 32768 to 65536)\n",
            "32768/32768 [==============================] - 15s 447us/sample - loss: 2.5145\n",
            "GENERATING SOME RANDOM TEXT FROM THE MODEL\n",
            "GENERATING TEXT WITH SEED: \n",
            "\"gods be good unto us!\n",
            "\n",
            "meneniu\"\n",
            "==TEMPERATURE: 0.2\n",
            "gods be good unto us!\n",
            "\n",
            "menenius ho he the the wore the the the the here wous hore wore the the the the the the het thare he the the the the wous the the the the the he the wore wore he the hone the hethe wous hind the the the the the the the tore th"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w3C-AIU18HFe"
      },
      "source": [
        "**NOTE**: Even after training has stopped you can still generate from the (partially trained) model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7HR4JXD3ZnX0",
        "colab": {}
      },
      "source": [
        "my_text = \"       the meaning of life is:\"  # Needs to be max_len characters\n",
        "print(len(my_text))\n",
        "sample_from_model(model, prev_text=my_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UbdLbn_xRe21"
      },
      "source": [
        "###IMPORTANT NOTES\n",
        "* Even if you stop training the model weights are persistant. If you resume training it will start where you left off. \n",
        "* To reset the weights, you need to recompile the model.\n",
        "* Sampling is **stochastic** (random), so you'll get new outputs every time you rerun the sampling code.\n",
        "\n",
        "### YOUR TASKS: \n",
        "* [**ALL**] Read the generations from your model in a funny voice to your neighbour.\n",
        "* [**ALL**] Increase `max_len` and regenerate the data and retrain the model.\n",
        " * What's the effect on training speed as you double `max_len`. Can you explain why?\n",
        " * Do you notice any effect on the quality of the model? Can you explain why?\n",
        "* [**ALL**] Set the max_len to be roughly the length of half a word; one word; two words... What kind of samples do these models generate? Explain how they go wrong.\n",
        "* [**ALL**] Change `embedding_dim` and the hidden size of the LSTM and observe the effect on training speed and quality.\n",
        "* [**INTERMEDIATE**] Change the dropout rates & retrain the model. \n",
        "  * What types of dropout do we get for recurrent models? \n",
        "  * What's the effect on the text quality?\n",
        "* [**ADVANCED**] Implement the \"teacher forcing\" training methodology, where the net must predict the entire output sequence shifted forward by one character, instead of just the next character. Compare the output of a model trained with teacher forcing versus the per-character model, given a similar training time. Is it fair to say that teacher forcing is a more efficient training methodology?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "REZzoUwjHkZW"
      },
      "source": [
        "##Further reading\n",
        "\n",
        "* https://distill.pub/2016/augmented-rnns/\n",
        "* https://distill.pub/2017/ctc/\n",
        "* https://algotravelling.com/en/machine-learning-fun-part-5/"
      ]
    }
  ]
}
