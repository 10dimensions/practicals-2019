{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practical 2b: Optimisation for Deep Learning",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2019/blob/master/optimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WiD0qVSnGN9",
        "colab_type": "text"
      },
      "source": [
        "# Practical 2b:  Optimisation for Deep Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this practical, we will take a *deep* dive into an essential part of deep learning, and machine learning in general, **optimisation**. We'll take a look at the tools that allow as to turn a random collection of weights into a state-of-the-art model for any number of applications. More specifically we'll implement a few standard optimisation algorithms for  finding the minimum of [Rosenbrock's banana function](https://en.wikipedia.org/wiki/Rosenbrock_function) and then we'll try them out on FashionMNIST.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "* Understand what optimisation algorithms are, and how they are used in the context of deep learning\n",
        "* Understand gradient descent, stochastic gradient descent, and mini-batch stochastic gradient descent\n",
        "* Understand the roles of batch size, learning rate and other hyper-parameters\n",
        "* Implement, using TF2.0, gradient descent and a few variations of it\n",
        "* Understand the strengths and weaknesses of the various optimisation algorithms covered in this practical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ_aoTV805AN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Imports (RUN ME!) { display-mode: \"form\" }\n",
        "\n",
        "# TODO: Swallow output\n",
        "!pip install tensorflow-gpu==2.0.0-beta0\n",
        "!pip -q install pydot_ng\n",
        "!pip -q install graphviz\n",
        "!apt install graphviz > /dev/null\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "display.clear_output()\n",
        "\n",
        "print(\"TensorFlow executing eagerly: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anRCKj57PRJT",
        "colab_type": "text"
      },
      "source": [
        "## Rosenbrock's Banana Function üçå\n",
        "\n",
        "In practice, when evaluating the performance of various optimisation algorithms and hyper-parameters, what we really care about is the performance on a wide range of real-world problems. However, we are easily not able to visualize what our algorithms are doing because the loss landscape for even simple neural networks trained on just about any real-world dataset will be high-dimensional. \n",
        "\n",
        "To solve this problem, we will use Rosenbrock's (Banana) Function as a playground for investigating how these optimisation algorithms work. The banana function is easy to visualize because it is a function that takes a 2D ($x$ and $y$) input and returns a scalar output. It is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "f(x,y) = (a-x)^2+b\\times(y-x^2)^2\n",
        "\\end{equation}\n",
        "\n",
        "where typical values for $a$ and $b$ are $1$ and $100$, respectively. For this prac we'll use $a = 1$ and $b = 20$. The global minimum of this function is at $(a, a^2)$ or $(1, 1)$ in our case.\n",
        "\n",
        "We can easily define this function using TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQwjMCKCSggF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rosenbrock_banana(x, y, a=1., b=20.):\n",
        "  return tf.math.pow(a-x,2.) + b*tf.math.pow(y-tf.math.pow(x,2.),2.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1tg42OeuaxH",
        "colab_type": "text"
      },
      "source": [
        "Let's try visualizing the üçå, first using a contour plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w18SilLGEehb",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Helper functions (double click to unhide/hide the code)\n",
        "\n",
        "def gen_2d_loss_surface(loss_func,\n",
        "             n_x=100, # number of discretization points along the x-axis\n",
        "             n_y=100, # number of discretization points along the x-axis\n",
        "             min_x=-2., max_x=2., # extreme points in the x-axis\n",
        "             min_y=-0.2, max_y=1.3 # extreme points in the y-axis\n",
        "            ):\n",
        "  \n",
        "  # create a mesh of points at which to evaluate our function\n",
        "  X,Y = np.meshgrid(np.linspace(min_x, max_x, n_x),\n",
        "                    np.linspace(min_y, max_y, n_y))\n",
        "  # evaluate the func at all of the points\n",
        "  Z = loss_func(X, Y).numpy()\n",
        "  \n",
        "  return X, Y, Z\n",
        "  \n",
        "def make_contour_plot(X, Y, Z, levels=None):\n",
        "  if levels==None:\n",
        "    # generate 20 levels on a log scale\n",
        "    levels = np.insert(np.logspace(0, 2.6, 20, True, base=10), 0, 0)\n",
        "    \n",
        "  fig = plt.figure(figsize=(9.84, 3))\n",
        "  ax = fig.gca()\n",
        "  \n",
        "  ax.contour(X, Y, Z, levels, alpha=0.5)\n",
        "  ax.contourf(X, Y, Z, levels, alpha=0.2)\n",
        "  ax.set_xlabel('x')\n",
        "  ax.set_ylabel('y')\n",
        "  \n",
        "  return fig, ax\n",
        "\n",
        "def make_surface_plot(X, Y, Z,\n",
        "                      elevation=0, azimuth_angle=0, levels=None):\n",
        "  \n",
        "  if levels==None:\n",
        "    # generate 20 levels on a log scale\n",
        "    levels = np.insert(np.logspace(0, 2.6, 20, True, base=10), 0, 0)\n",
        "    \n",
        "  fig = plt.figure(figsize=(10,6))\n",
        "  ax = fig.gca(projection='3d')\n",
        "  ax.view_init(elevation, azimuth_angle)\n",
        "  ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.2)\n",
        "  ax.contour(X, Y, Z, levels, cmap='viridis', alpha=0.5)\n",
        "  \n",
        "  ax.set_xlabel('x')\n",
        "  ax.set_ylabel('y')\n",
        "  \n",
        "  return fig, ax\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFU6ljswmCE2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "X, Y, Z = gen_2d_loss_surface(rosenbrock_banana)\n",
        "fig, ax = make_contour_plot(X, Y, Z)\n",
        "\n",
        "# add a marker to show the minimum\n",
        "ax.plot(1, 1, 'r*', ms=20, label='minimum') \n",
        "ax.legend()\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuOXyVa5COkv",
        "colab_type": "text"
      },
      "source": [
        "And now with a surface plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeMlgRP1TObh",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title {run: \"auto\"}\n",
        "\n",
        "elevation = 39 #@param {type:\"slider\", min:0, max:360, step:1}\n",
        "azimuth_angle = 110 #@param {type:\"slider\", min:0, max:360, step:1}\n",
        "  \n",
        "X, Y, Z = gen_2d_loss_surface(rosenbrock_banana)\n",
        "fig, ax = make_surface_plot(X, Y, Z, elevation, azimuth_angle)\n",
        "\n",
        "ax.plot([1], [1], 'r*', zs=[0], zdir='z', ms=20, label='minimum')\n",
        "ax.legend()\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwx9E5RCssMp",
        "colab_type": "text"
      },
      "source": [
        "As you can see, this is called the *banana* function because it contains a banana-shaped valley. Within the valley, we have a global minimum. Finding the valley is relatively easy, but finding the global minimum is difficult, which makes this a useful function for testing optimisation algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag4YyzcSytVG",
        "colab_type": "text"
      },
      "source": [
        "## Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH14orLU4aUj",
        "colab_type": "text"
      },
      "source": [
        "TODO: Steal some content from *How to build your own TF*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZyrDzor2uCC",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAZp7A042xH_",
        "colab_type": "text"
      },
      "source": [
        "Gradient descent is the most simple of learning algorithms commonly used in deep learning. However, it not only gives excellent results in many cases but also forms the basis for many other powerful optimization methods such as Momentum, RMSProp, and Adam which we will look at later in this practical. Mathematically we can describe gradient descent as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{Œ∏}= \\mathbf{Œ∏} ‚àí\\eta \\times \\nabla_\\mathbf{Œ∏} J(\\mathbf{Œ∏})\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mathbf{Œ∏}$ are the parameters of the model, $\\eta$ is the learning rate, $J(\\mathbf{Œ∏})$ is the loss function, and $\\nabla_\\theta J(\\mathbf{Œ∏})$ is the **gradient** of the loss with respect to the parameters. What this equation tells us is that to update each of the parameters scale the gradient for each parameter by the learning rate and subtract it from the corresponding parameter, or in pseudo code:\n",
        "\n",
        "```\n",
        "for _ in range(epochs):\n",
        "  grad = calc_grad(loss_func, data, params)\n",
        "  params = params - learning_rate * grad\n",
        "```\n",
        "\n",
        "You might have noticed that we are sweeping a lot of critical details under the rug here! Firstly we are assuming that we can easily calculate the gradients using some `calc_grad` function, and secondly, we are ignoring the issue of batch size. \n",
        "\n",
        "Luckily for us, TensorFlow addresses the first detail thanks to **automatic differentiation** (AD). With AD calculating the gradients is about as simple as calling a `calc_grad` function, which means that we do not need to worry about the details of *how* to calculate the gradients. We don't need to think much about implementing derivatives for each of our operations, or the backpropagation algorithm, for example. If you want to know more about how this all works, you should check out the *Build your own TensorFlow* tutorial.\n",
        "\n",
        "In practice, batch size is simply a hyper-parameter that we can tune. However, there are three cases that are worth knowing about:\n",
        "\n",
        "1.   Batch size of $n$, where $n$ is the number of examples, also known as *Batch Gradient Descent*. In this case, all of the data is used to calculate the gradient at each step, which results in the most accurate estimate of the gradient and is guaranteed to converge to the global minimum, for convex optimisation surfaces, or a local minimum, for non-convex surfaces. However, this comes at the cost of not applying to online learning, where we get new examples during training.\n",
        "2.   Batch size of 1, called *Stochastic Gradient Descent* (SGD). In this case, the estimate of the gradient is very noisy, and we are not gauranteed to find a minimum (local or global). However, in practice SGD, still performs very well. SGD also allows for online learning. It also turns out that having noisy estimates of the gradient acts as a form of regularisation to prevent over-fitting. Finally, because we are performing gradient descent with one example as a time, we need much less memory, which can be a significant issue when using batch gradient descent.\n",
        "3.   Batch size of $m < n$, called *Mini-batch Gradient Descent*, which is a compromise between batch and stochastic gradient descent. We still have *some* noise in the gradient estimate, and we can tune the batch size to make good use of memory, but the variance of the gradient estimate is greatly reduced which leads to better convergence to local or global minima.\n",
        "\n",
        "In deep learning, we almost always use mini-batch gradient descent. However, it is often referred to as SGD.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl8cnkgCSN8_",
        "colab_type": "text"
      },
      "source": [
        "### Optional extra reading: choosing a batch size\n",
        "\n",
        "You might be wondering how one chooses which batch size to use for a given problem.\n",
        "\n",
        "One approach is to choose as big a batch size as possible. The reason we might want to do this is that a larger batch size means that our model will train faster. This speedup is because modern computer hardware, especially GPUs, is designed with parallelism in mind. In other words, by having a larger batch size, we can take better advantage of our hardware. The issue with this philosophy is that, especially with images, audio, and other multimedia data often used in deep learning, we quickly reach the memory limits of our hardware. So in practice we often choose the largest batch size that will fit into memory.\n",
        "\n",
        "Another reason to choose a large batch size is that, [according to some research](https://arxiv.org/pdf/1803.09820.pdf), this allows you to select a higher learning rate, which in turn means that your model will train faster.\n",
        "\n",
        "On the other hand, in practice choosing too large, a batch size can lead to training divergence, which means that we still have to tune the batch size.\n",
        "\n",
        "You can read more about these issues in this [blog post](https://blog.janestreet.com/does-batch-size-matter/) as well as the paper linked above ‚Äî the paper, in particular, discusses strategies for choosing various hyper-parameters related to optimisation in deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqzkyzvvSUxY",
        "colab_type": "text"
      },
      "source": [
        "### Implementing SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF-1lI-S0x7r",
        "colab_type": "text"
      },
      "source": [
        "SGD is incredibly simple to implement, and as you'll see later can perform very well!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXEq8950yU4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SGD_update(params, grads, states, hyper_params):\n",
        "  # learning_rate=0.01\n",
        "  # SGD doesn't have any state, however, the algorithms\n",
        "  # we will look at later do!\n",
        "  for param, grad in zip(params, grads):\n",
        "    param.assign_sub(hyper_params['lr']*grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHavR63e1bQ1",
        "colab_type": "text"
      },
      "source": [
        "And now we can visualize each step of the SGD optimization:\n",
        "\n",
        "(Take a look at the code and make sure that you understand it. Do you see how the code relates to the pseudo-code and equation above? For example, where do you find `calc_grad`? Once you understand the code, you should hide it again to make it easier to change the sliders and see the results.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqnxFxuyEweK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Helper functions (double click to unhide/hide the code)\n",
        "\n",
        "def optimize_banana(update_func, params, states, hyper_params):\n",
        "  # plot the loss surface, minimum value and starting point\n",
        "  X, Y, Z = gen_2d_loss_surface(rosenbrock_banana)\n",
        "  fig, ax = make_contour_plot(X, Y, Z)\n",
        "  ax.plot(1, 1, 'r*', ms=20, label='minimum') \n",
        "  ax.plot(start_x, start_y, 'b*', ms=20, label='start')\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "      # we are trying to minimize is the output of the üçå func\n",
        "      loss = rosenbrock_banana(x, y) \n",
        "    # calculate the gradients of the loss with respect to the params\n",
        "    grads = tape.gradient(loss, params)\n",
        "\n",
        "    # save the old x and y values for the plot\n",
        "    old_x = params[0].numpy()\n",
        "    old_y = params[1].numpy()\n",
        "\n",
        "    # update the parameters using SGD\n",
        "    update_func(params, grads, states, hyper_params)\n",
        "\n",
        "    # plot the change in x and y for each update step\n",
        "    ax.annotate('', xy=(x.numpy(), y.numpy()),\n",
        "                xytext=(old_x, old_y),\n",
        "              arrowprops={'arrowstyle': '->', 'color': 'k', 'lw': 1},\n",
        "                   va='center', ha='center')  \n",
        "\n",
        "  ax.plot(x.numpy(), y.numpy(), 'g*', ms=20, label='end')\n",
        "  ax.legend()\n",
        "\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veRWE23R4TSi",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Double click to unhide/hide the code {run: \"auto\"}\n",
        "start_x = -0.4 #@param {type:\"slider\", min:-2, max:2, step:0.1}\n",
        "start_y = 0.63334 #@param {type:\"slider\", min:-0.26666, max:1.0666, step:0.1}\n",
        "learning_rate = 0.018 #@param {type:\"slider\", min:0, max:0.02, step:0.0005}\n",
        "epochs = 110 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "x = tf.Variable(start_x, dtype='float32')  \n",
        "y = tf.Variable(start_y, dtype='float32')\n",
        "params = [x, y]\n",
        "states = []\n",
        "hyper_params = {\"lr\": learning_rate}\n",
        "\n",
        "optimize_banana(SGD_update, params, states, hyper_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j77_Fe02wqe",
        "colab_type": "text"
      },
      "source": [
        "**Partner Excercise:** Tweak the starting position (x and y), as well as the learning rate and the number of epochs. See if you can get to the global minimum. What do you notice about the behaviour of SGD in the üçå valley?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMJUFyny4EnB",
        "colab_type": "text"
      },
      "source": [
        "## SGD with Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzDIrG6w4yQ9",
        "colab_type": "text"
      },
      "source": [
        "Hopefully, you've noticed a bit of an issue with SGD, which is that when the gradient is small (i.e. in the üçå) the changes to the parameters become very small and progress towards the minimum is very slow. One solution to this problem is to add a momentum term to our optimization step:\n",
        "\n",
        "\\begin{align}\n",
        "\\Delta \\mathbf{Œ∏} &= Œ≥\\Delta \\mathbf{Œ∏}+Œ∑‚àá_\\mathbf{Œ∏}J(\\mathbf{Œ∏}) \\\\\n",
        "\\mathbf{Œ∏} &= \\mathbf{Œ∏}‚àí\\Delta \\mathbf{Œ∏}\n",
        "\\end{align}\n",
        "\n",
        "where $\\Delta \\mathbf{Œ∏}$ is the change in parameters $\\mathbf{Œ∏}$ at each time step and is made up of a mixture between the gradients and at given time step and the change from the previous step. $Œ≥$ is called the *momentum* term, and $\\eta$ is called the learning rate, as before.\n",
        "\n",
        "The reason that this method is called *momentum* is that we can compare it to SGD as follows:\n",
        "\n",
        "> gradient descent is a man walking down a hill. He follows the steepest path downwards; his progress is slow, but steady. Momentum is a heavy ball rolling down the same hill. The added inertia acts both as a smoother and an accelerator, dampening oscillations and causing us to barrel through narrow valleys, small humps and local minima. \n",
        "\n",
        "In other words, the momentum term speeds up optimisation if the direction of change stays the same or similar and reduces oscillations when the direction of change goes back and forth. \n",
        "\n",
        "We can also describe momentum with some simple pseudo-code:\n",
        "\n",
        "```\n",
        "change = 0\n",
        "for _ in range(epochs):\n",
        "  grad = calc_grad(loss_func, data, params)\n",
        "  change = momentum*change + learning_rate*grad\n",
        "  params = params - change\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBGPpJHn4vRW",
        "colab_type": "text"
      },
      "source": [
        "### Implementing Momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSJmsRTKg86_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def momentum_update(params, grads, states, hyper_params):\n",
        "  # learning_rate=0.01, momentum=0.9\n",
        "  changes = states['changes']\n",
        "  for param, grad in zip(params, grads):\n",
        "    changes[param].assign(hyper_params['momentum']*changes[param] +\n",
        "                          hyper_params['lr']*grad)\n",
        "    param.assign_sub(changes[param])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDyzTYPlB31-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Double click to unhide/hide the code{run: \"auto\"}\n",
        "start_x = -0.4 #@param {type:\"slider\", min:-2, max:2, step:0.1}\n",
        "start_y = 0.63334 #@param {type:\"slider\", min:-0.26666, max:1.0666, step:0.1}\n",
        "lr = 0.001 #@param {type:\"slider\", min:0, max:0.02,step:0.0005}\n",
        "momentum = 0.23 #@param {type:\"slider\", min:0, max:0.99, step:0.01}\n",
        "epochs = 119 #@param {type:\"slider\", min:1, max:150,step:1}\n",
        "\n",
        "x = tf.Variable(start_x, dtype='float32')  \n",
        "y = tf.Variable(start_y, dtype='float32')\n",
        "params = [x, y]\n",
        "changes = {param: tf.Variable(0., dtype='float32') for param in params}\n",
        "hyper_params = {\"lr\": learning_rate, \"momentum\": momentum}\n",
        "states = {\"changes\": changes}\n",
        "\n",
        "optimize_banana(momentum_update, params, states, hyper_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8popkDD1IVo",
        "colab_type": "text"
      },
      "source": [
        "**Partner Excercise:** Play around with the various parameters and see if you can get to the minimum. Compare the performance of momentum in the üçå to that of SGD.\n",
        "\n",
        "**Partner Excercise:** Why do we see oscillations for high enough momentum values?\n",
        "\n",
        "**Partner Excercise:** Do you see any relationship between the amount of momentum and the learning rate?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IujnQlvqFpgN",
        "colab_type": "text"
      },
      "source": [
        "## RMSProp (Root Mean Square Propagation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5McgKZaFpBy",
        "colab_type": "text"
      },
      "source": [
        "One problem with both Momentum and SGD is that each parameter has the same fixed learning rate. However, we can imagine that:\n",
        "\n",
        "\n",
        "1.   Each weight might not need to vary by the same amount.\n",
        "2.   The amount we want to change each parameter changes throughout the optimisation process might change. \n",
        "\n",
        "**Excercise:** can you think of examples of when these two cases might apply?\n",
        "\n",
        "RMSProp is a method that addresses these issues. It can be described with the following formulae:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{v} &= Œ≥\\mathbf{v}+(1 - Œ≥)(‚àá_\\mathbf{Œ∏}J(\\mathbf{Œ∏}))^2 \\\\\n",
        "\\mathbf{Œ∏} &= \\mathbf{Œ∏}‚àí\\frac{Œ∑}{\\sqrt{\\mathbf{v}} + œµ}‚àá_\\mathbf{Œ∏}J(\\mathbf{Œ∏})\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf v$ is an estimate of the square of the gradient for each parameter, calculated using a rolling average, $Œ≥$ is a forgetting factor for the rolling average, and $œµ$ is a small number added for numerical stability. \n",
        "\n",
        "In words, RMSProp is scaling down the learning rate for each gradient by rolling average of the most recent gradients for that parameter. Importantly **each parameter has its own learning rate, which changes over time**.\n",
        "\n",
        "We can also describe RMSPRop using pseudo code:\n",
        "\n",
        "```\n",
        "average = 0\n",
        "for _ in range(epochs):\n",
        "  grad = calc_grad(loss_func, data, params)\n",
        "  average = gamma*average + (1 - gamma)*pow(grad, 2)\n",
        "  params = params - learning_rate/sqrt(average)*grad\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woMqJYFbdGWO",
        "colab_type": "text"
      },
      "source": [
        "### Implementing RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuPoytCSdztD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RMSProp_update(params, grads, states, hyper_params):\n",
        "  # learning_rate=0.001, mixing_prop=0.9, eps=1e-8\n",
        "  averages = states['averages']\n",
        "  for param, grad in zip(params, grads):\n",
        "    averages[param].assign(hyper_params['gamma']*averages[param] +\n",
        "                           (1 - hyper_params['gamma'])*tf.math.pow(grad, 2))\n",
        "    param.assign_sub(hyper_params['lr']/(tf.sqrt(averages[param]) + hyper_params['eps'])*grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuqG4shffumO",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Double click to unhide/hide the code {run: \"auto\"}\n",
        "start_x = -0.4 #@param {type:\"slider\", min:-2, max:2, step:0.1}\n",
        "start_y = 0.63334 #@param {type:\"slider\", min:-0.26666, max:1.0666, step:0.1}\n",
        "lr = 0.012 #@param {type:\"slider\", min:0, max:0.02,step:0.0005}\n",
        "gamma = 0.9 #@param {type:\"slider\", min:0.01, max:0.99, step:0.01}\n",
        "epochs = 150 #@param {type:\"slider\", min:1, max:150,step:1}\n",
        "\n",
        "x = tf.Variable(start_x, dtype='float32')  \n",
        "y = tf.Variable(start_y, dtype='float32')\n",
        "params = [x, y]\n",
        "averages = {param: tf.Variable(0., dtype='float32') for param in params}\n",
        "states = {\"averages\": averages}\n",
        "hyper_params = {\"lr\": learning_rate, \"gamma\": gamma, \"eps\": 1e-8}\n",
        "  \n",
        "optimize_banana(RMSProp_update, params, states, hyper_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwjIfkp16Etl",
        "colab_type": "text"
      },
      "source": [
        "**Partner Excercise:** Play around with the various parameters and see if you can get to the minimum. Compare the performance of RMSProp with momentum and SGD, particularly in the üçå."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRymHjfYDagF",
        "colab_type": "text"
      },
      "source": [
        "## Adam (Adaptive moment estimation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOtreVpxFE4X",
        "colab_type": "text"
      },
      "source": [
        "Adam combines the ideas of momentum and adaptive learning rates that we have explored above. More specifically, in addition to storing the rolling averages of the *squared* gradients and using them to control the learning rate for each parameter, like RMSProp, it also stores the rolling averages of the gradients themselves and uses them like momentum. Mathematically, we can describe Adam as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf m &= Œ≤_1\\mathbf m+(1‚àíŒ≤_1)‚àá_\\mathbf{Œ∏}J(\\mathbf{Œ∏}) \\\\\n",
        "\\mathbf v &= Œ≤_2\\mathbf v+(1‚àíŒ≤_2)(‚àá_\\mathbf{Œ∏}J(\\mathbf{Œ∏}))^2 \\\\\n",
        "\\mathbf{\\hat{m}} &= \\frac{\\mathbf m}{1‚àíŒ≤_1^t} \\\\\n",
        "\\mathbf{\\hat{v}} &= \\frac{\\mathbf v}{1‚àíŒ≤_2^t} \\\\\n",
        "\\mathbf{Œ∏} &= \\mathbf{Œ∏}‚àí\\frac{Œ∑}{\\sqrt{\\mathbf{\\hat{v}}} + œµ}\\mathbf{\\hat{m}}\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf{m}$ is a rolling estimate of the gradient, $\\mathbf{v}$ is a rolling estimate of the squared gradient, and $\\mathbf{\\hat{m}}$ and $\\mathbf{\\hat{v}}$ are bias-corrected estimates. As before $Œ∑$ and $œµ$ are the learning rate and a numerical stability term.\n",
        "\n",
        "The name Adam comes from the fact that $\\mathbf{m}$ and $\\mathbf{v}$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradient. The reason that we need bias corrected versions of the estimates is that they are initialized to be zero vectors, which means that when the training starts, the estimates are biased towards zero. This problem is especially relevant when $Œ≤_1$ and $Œ≤_2$ are close to 1.\n",
        "\n",
        "The pseudo code for Adam is slightly longer than the other methods we've looked at but should be reasonably simple to understand. If something doesn't make sense, go back and look at momentum and RMSProp.\n",
        "\n",
        "```\n",
        "first_moment = 0\n",
        "second_moment = 0\n",
        "t = 0\n",
        "for _ in range(epochs):\n",
        "  t = t + 1\n",
        "  grad = calc_grad(loss_func, data, params)\n",
        "  first_moment = mixing_prop1*first_moment + (1 - beta1)*grad\n",
        "  second_moment = mixing_prop2*second_moment + (1 - beta2)*pow(grad, 2)\n",
        "  first_moment_unbiased = first_moment/(1 - pow(beta1, t))\n",
        "  second_moment_unbiased = second_moment/(1 - pow(beta2, t))\n",
        "  params = params - learning_rate/sqrt(second_moment_unbiased)*first_moment_unbiased\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgvb-C-0q4If",
        "colab_type": "text"
      },
      "source": [
        "### Implementing Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mnm5L8ODbD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Adam_update(params, grads, states, hyper_params):\n",
        "  #learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8\n",
        "  t = states[\"t\"]\n",
        "  t.assign_add(1.0)  \n",
        "  first_moments = states[\"first_moments\"]\n",
        "  second_moments = states[\"second_moments\"]\n",
        "  \n",
        "  for param, grad in zip(params, grads):\n",
        "    # come back here later!\n",
        "    # the rest of this function will be removed and participants will implement as an excercise\n",
        "    first_moments[param].assign(hyper_params[\"beta1\"]*first_moments[param] +\n",
        "                                (1 - hyper_params[\"beta1\"])*grad)\n",
        "    second_moments[param].assign(hyper_params[\"beta2\"]*second_moments[param] +\n",
        "                                 (1 - hyper_params[\"beta2\"])*tf.math.pow(grad, 2))\n",
        "    \n",
        "    first_moment_unbiased = first_moments[param]/(1 - tf.pow(beta1, t))\n",
        "    second_moment_unbiased = second_moments[param]/(1 - tf.pow(beta2, t))\n",
        "    \n",
        "    param.assign_sub(hyper_params[\"lr\"]/(tf.sqrt(second_moment_unbiased) +\n",
        "                                    hyper_params[\"eps\"])*first_moment_unbiased)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCPLzIHDf81t",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Double click to unhide/hide the code {run: \"auto\"}\n",
        "start_x = -0.4 #@param {type:\"slider\", min:-2, max:2, step:0.1}\n",
        "start_y = 0.63334 #@param {type:\"slider\", min:-0.26666, max:1.0666, step:0.1}\n",
        "lr = 0.016 #@param {type:\"slider\", min:0, max:0.02,step:0.0005}\n",
        "beta1 = 0.96 #@param {type:\"slider\", min:0.01, max:0.99, step:0.01}\n",
        "beta2 = 0.907 #@param {type:\"slider\", min:0.01, max:0.999, step:0.001}\n",
        "epochs = 150 #@param {type:\"slider\", min:1, max:150,step:1}\n",
        "\n",
        "x = tf.Variable(start_x, dtype='float32')  \n",
        "y = tf.Variable(start_y, dtype='float32')\n",
        "params = [x, y]\n",
        "first_moments = {param: tf.Variable(0., dtype='float32')\n",
        "                 for param in params}\n",
        "second_moments = {param: tf.Variable(0., dtype='float32')\n",
        "                                     for param in params}\n",
        "t = tf.Variable(0., dtype='float32')\n",
        "states = {\"first_moments\": first_moments, \n",
        "          \"second_moments\": second_moments, \"t\": t}\n",
        "hyper_params = {\"lr\": learning_rate, \"beta1\": beta1, \"beta2\": beta2, \"eps\": 1e-8}\n",
        "\n",
        "optimize_banana(Adam_update, params, states, hyper_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbDpI0ehaZC2",
        "colab_type": "text"
      },
      "source": [
        "## Learning Rate Decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Deoi1TZcdF9-",
        "colab_type": "text"
      },
      "source": [
        "One of the advantages of methods such as Adam and RMSProp is that the effective learning rate for each parameter is no longer a constant during training. This behaviour is desirable because we often want to reduce the learning rate as we near the global minimum so that we do not shoot past it. Another simple strategy for solving this problem is *learning rate decay*. With learning rate decay, we progressively reduce the learning rate during the training process. For example, we might decay our learning rate as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "  Œ∑ = Œ∑ \\times \\frac{1}{1 + \\delta \\times t}\n",
        "\\end{equation}\n",
        "\n",
        "where $Œ∑$ is the learning rate, $\\delta$ is the decay rate, and $t$ is the number of the current training epoch. This method will give us a learning rate that looks something like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmzu-nI5iR1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Œ∑ = 0.01\n",
        "epochs = 100\n",
        "Œ¥ = Œ∑/epochs\n",
        "Œ∑s = [Œ∑]\n",
        "for t in range(epochs):\n",
        "  Œ∑s.append(Œ∑s[t]*1/(1 + Œ¥*t))\n",
        "  \n",
        "plt.plot(Œ∑s)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVZ-scjAj7Zr",
        "colab_type": "text"
      },
      "source": [
        "In practice, while we do often use the simple decay scheme shown above, it is also common to use more complicated *learning rate schedules* such as exponential decay and step decay:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRY6xZn_mVHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exponential decay\n",
        "Œ∑ = 0.01\n",
        "epochs = 100\n",
        "Œ¥ = 0.01\n",
        "Œ∑s = []\n",
        "for t in range(epochs):\n",
        "  Œ∑s.append(Œ∑*np.e**(-Œ¥*t))\n",
        "  \n",
        "plt.plot(Œ∑s)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4_-JDBknfGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# step decay\n",
        "Œ∑ = 0.01\n",
        "epochs = 100\n",
        "epochs_wait = 10\n",
        "Œ¥ = 0.5\n",
        "Œ∑s = []\n",
        "for t in range(epochs):\n",
        "  Œ∑s.append(Œ∑*Œ¥**np.floor((1+t)/epochs_wait))\n",
        "  \n",
        "plt.plot(Œ∑s)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKTj5p9UprUm",
        "colab_type": "text"
      },
      "source": [
        "Any of these learning rate decay methods are compatible with the optimisation methods described above. The choices of whether or not to use learning rate decay and if so, which method to use are hyper-parameters that we can tune.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRtm9JmFD3Ks",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all into practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSopLTJPD8Qm",
        "colab_type": "text"
      },
      "source": [
        "TODO:\n",
        "\n",
        "* practical advice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klDdVCbiP0Cn",
        "colab_type": "text"
      },
      "source": [
        "Putting all of this into practice is very simple! Keras provides us with a high-level API that makes using any of the optimization methods or learning rate schedules as easy as changing a single line of code. Of course, if you are defining a custom training loop using `tf.GradientTape` then the code we've used above can easily be converted to work for any model and dataset. \n",
        "\n",
        "One of the advantages of using Keras is that it provides us with reasonable default values for all of the hyper-parameters of the optimization algorithms. \n",
        "\n",
        "Let's use Keras to train a simple MLP on FashionMNIST so that we can compare the optimization algorithms we've looked at in a more realistic setting.\n",
        "\n",
        "As a quick reminder, FashionMNIST contains 28x28 grayscale images from 10 different types of clothing. Let's take a quick look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQxsa6H0SCvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "text_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "\n",
        "    img_index = np.random.randint(0, 50000)\n",
        "    plt.imshow(train_images[img_index], cmap=\"gray_r\")\n",
        "    plt.xlabel(text_labels[train_labels[img_index]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SNA2x5ETHhF",
        "colab_type": "text"
      },
      "source": [
        "Before we train on the data, we want to do a little pre-processing. We won't go into detail about how and why we are doing the pre-processing, but if you want to know more, you should take a look at the *Deep Feedforward Networks* practical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YPa_8xcTGn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "# Divide image values and cast to float so that they end up as a floating point number between 0 and 1\n",
        "train_ds = train_ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, tf.cast(y, tf.int32)))\n",
        "# Shuffle the examples.\n",
        "train_ds = train_ds.shuffle(buffer_size=batch_size * 10)\n",
        "# Now \"chunk\" the examples into batches\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "test_ds = test_ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, tf.cast(y, tf.int32)))\n",
        "test_ds = test_ds.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKzrbhRTv2V",
        "colab_type": "text"
      },
      "source": [
        "Now let's define a simple MLP:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI1Slx66S78W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_mlp():\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = build_mlp()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuKDP3YYUXDW",
        "colab_type": "text"
      },
      "source": [
        "And finally, lets train the MLP using a few different optimizers and compare the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI1OL9Orp1Hl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Helper functions (double click to unhide/hide the code)\n",
        "\n",
        "def make_loss_plots(losses):\n",
        "  plt.close()\n",
        "  for label, loss_vals in losses.items():\n",
        "    plt.plot(loss_vals, label=label)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-4f4X8XVH2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {}\n",
        "\n",
        "# SGD\n",
        "model = build_mlp()\n",
        "model.compile(optimizer='sgd', \n",
        "              # we can use the string shortcut if we \n",
        "              # don't want to change the hyper-parameters\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sgd_hist = model.fit(train_ds, epochs=5,\n",
        "          validation_data=test_ds)\n",
        "\n",
        "losses['SGD'] = sgd_hist.history[\"val_loss\"]\n",
        "make_loss_plots(losses)\n",
        "\n",
        "\n",
        "\n",
        "# Momentum\n",
        "model = build_mlp()\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01, momentum=0.0),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "momentum_hist = model.fit(train_ds, epochs=5,\n",
        "          validation_data=test_ds)\n",
        "\n",
        "losses['Momentum'] = momentum_hist.history[\"val_loss\"]\n",
        "make_loss_plots(losses)\n",
        "\n",
        "# RMSProp\n",
        "model = build_mlp()\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9),\n",
        "              # rho is the symbol used for the forget factor in Keras\n",
        "              # we used gamma (Œ≥) in this practical\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "rmsprop_hist = model.fit(train_ds, epochs=5,\n",
        "          validation_data=test_ds)\n",
        "\n",
        "losses['RMSProp'] = rmsprop_hist.history[\"val_loss\"]\n",
        "make_loss_plots(losses)\n",
        "\n",
        "# Adam\n",
        "model = build_mlp()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "adam_hist = model.fit(train_ds, epochs=5,\n",
        "          validation_data=test_ds)\n",
        "\n",
        "losses['Adam'] = adam_hist.history[\"val_loss\"]\n",
        "make_loss_plots(losses)\n",
        "\n",
        "# SGD with learning rate decay\n",
        "model = build_mlp()\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.05, momentum=0.0, decay=0.9),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sgd_decay_hist = model.fit(train_ds, epochs=5,\n",
        "          validation_data=test_ds)\n",
        "\n",
        "losses['SGD with decay'] = sgd_decay_hist.history[\"val_loss\"]\n",
        "make_loss_plots(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3gMgKgltU23",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Play with the parameters of the various optimizers and see how that affects this particular problem.\n",
        "\n",
        "**Note:** There is an element of randomness each time we train our models; this means that we should be careful when making judgements about which methods are best for this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5aoZHykLYgW",
        "colab_type": "text"
      },
      "source": [
        "## Optional extra reading: second order methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezeyeKFOOUr1",
        "colab_type": "text"
      },
      "source": [
        "TODO: discus methods like BFGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlwOOILYntwZ",
        "colab_type": "text"
      },
      "source": [
        "## Optional extra reading: weight decay vs L2 norm regularization\n",
        "\n",
        "TODO: weight decay is a way of implementing L2 norm regularization in the optimizer\n",
        "\n",
        "TODO: discuss AdamW?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNGBSO8ya2H-",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion: what method should you use?\n",
        "\n",
        "There are no hard rules for which methods you should use. It will always depend on your particular model and dataset. However, some guidelines are worth keeping in mind:\n",
        "\n",
        "1. Adam typically works very well in a large number of settings and is usually a good first choice.\n",
        "2. RMSProp can often outperform Adam for RNNs as well as in RL. If you are working in either of these domains, then it might be worth trying RMSProp.\n",
        "3. Good old SGD and SGD with momentum often work just as well as more sophisticated methods like Adam. Don't think that they aren't worth trying out just because they are simple.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7UcgvT_y5_C",
        "colab_type": "text"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "1. **[Optional, Advanced]** Implement Nesterov Momentum. You can read more about it [here](http://ruder.io/optimizing-gradient-descent/).\n",
        "2. **[All]** Combine the implementations for momentum and RMSProp to implement Adam. Experiment with the parameters of Adam and compare it to the previous methods we have looked at.\n",
        "3. **[Optional, Advanced]** Implement Nadam. You can read more about it [here](http://ruder.io/optimizing-gradient-descent/).\n",
        "4. **[All]** Augment any of the optimization methods with learning rate decay. You can choose any of the learning rate decay methods. Play around with the parameters. Compare the performance of the optimization method with and without learning rate decay.\n",
        "5. **[Intermediate]** Implement the other two learning rate decay methods and compare all three decay methods with one another.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A1GmqWZL_jW",
        "colab_type": "text"
      },
      "source": [
        "## Extra resources\n",
        "\n",
        "* A [blog post](http://fa.bianp.net/teaching/2018/eecs227at/gradient_descent.html) from Fabian Pedregosa on gradient descent [**Highly Recommended**].\n",
        "* [Distil.pub post](https://distill.pub/2017/momentum/) by Gabriel Goh on why momentum works [**Highly Recommended**].\n",
        "* [Sebastian Ruder's blog](http://ruder.io/optimizing-gradient-descent/) on gradient descent algorithms.\n",
        "* Deep Dive into Deep Learning chapter on [Optimization Algorithms](http://d2l.ai/chapter_optimization/index.html).\n",
        "* Keras optimizer [docs](https://keras.io/optimizers/).\n"
      ]
    }
  ]
}